"""
LLM Client for BGBG AI Server
Handles communication with GMS API (SSAFY Anthropic proxy)
"""

import asyncio
import httpx
from typing import Dict, List, Optional, Any
from enum import Enum
from loguru import logger

from src.config.settings import get_settings


class LLMProvider(Enum):
    GMS = "gms"
    MOCK = "mock"


class LLMClient:
    """Client for interacting with GMS API"""
    
    def __init__(self):
        self.settings = get_settings()
        self.gms_available = False  

    def _is_valid_api_key(self, api_key: Optional[str]) -> bool:
        """Check if API key is valid (not empty or placeholder)"""
        if not api_key:
            return False
        
        # Check for common placeholder patterns
        placeholder_patterns = [
            "YOUR_", "ENTER_", "INSERT_", "ADD_", "REPLACE_",
            "_HERE", "_KEY_HERE", "sk-...", "sk-xxx", "your-", "placeholder"
        ]
        
        api_key_upper = api_key.upper()
        for pattern in placeholder_patterns:
            if pattern in api_key_upper:
                return False
        
        # Check minimum length (most API keys are at least 20 characters)
        # Exception for Gemini keys which can be shorter but valid
        if api_key.startswith("AIza"):
            return len(api_key) >= 35  # Gemini API keys are typically 39 characters
        
        return len(api_key) >= 20

    async def initialize(self):
        """Initialize GMS API client"""
        try:
            logger.info("Initializing GMS API client...")
            logger.info(f"ğŸ” Debug - Mock Responses: {self.settings.ai.MOCK_AI_RESPONSES}")
            
            # Check GMS API key
            if self._is_valid_api_key(self.settings.ai.GMS_API_KEY):
                # Test GMS API connection
                await self._test_gms_connection()
                self.gms_available = True
                logger.info("âœ… GMS API client initialized")
            else:
                logger.warning("âš ï¸ GMS API key not configured or invalid")
                self.gms_available = False
            
            if not self.gms_available:
                logger.warning("âŒ GMS API not available - using mock responses")
                logger.info("ğŸ’¡ To use GMS API, configure valid GMS_API_KEY in .env file")
                
        except Exception as e:
            logger.error(f"Failed to initialize GMS API client: {e}")
            self.gms_available = False
            logger.info("âŒ Falling back to mock responses")
    
    async def generate_completion(
        self,
        prompt: str,
        system_message: Optional[str] = None,
        max_tokens: int = 1000,
        temperature: float = 0.7,
        provider: Optional[LLMProvider] = None,
        model: Optional[str] = None
    ) -> str:
        """Generate text completion using GMS API"""
        
        if self.settings.ai.MOCK_AI_RESPONSES or not self.gms_available:
            return await self._mock_completion(prompt)
        
        try:
            return await self._gms_completion(prompt, system_message, max_tokens, temperature)
        except Exception as e:
            logger.error(f"GMS API completion failed: {e}")
            logger.warning("Falling back to mock completion")
            return await self._mock_completion(prompt)
    
    async def _test_gms_connection(self):
        """Test GMS API connection"""
        try:
            headers = {
                "Content-Type": "application/json",
                "x-api-key": self.settings.ai.GMS_API_KEY,
                "anthropic-version": "2023-06-01"
            }
            
            test_data = {
                "model": self.settings.ai.GMS_DEV_MODEL,
                "max_tokens": 10,
                "messages": [{"role": "user", "content": "Hi"}]
            }
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(
                    f"{self.settings.ai.GMS_BASE_URL}/messages",
                    headers=headers,
                    json=test_data
                )
                response.raise_for_status()
                logger.info("âœ… GMS API connection test successful")
                
        except Exception as e:
            logger.error(f"âŒ GMS API connection test failed: {e}")
            raise
    
    async def _gms_completion(
        self,
        prompt: str,
        system_message: Optional[str],
        max_tokens: int,
        temperature: float
    ) -> str:
        """Generate completion using GMS (SSAFY Anthropic proxy)"""
        try:
            logger.debug("ğŸ”„ Using GMS API for completion")
            
            headers = {
                "Content-Type": "application/json",
                "x-api-key": self.settings.ai.GMS_API_KEY,
                "anthropic-version": "2023-06-01"
            }
            
            # ê°œë°œ/í”„ë¡œë•ì…˜ ëª¨ë¸ ì„ íƒ
            model = self.settings.ai.GMS_DEV_MODEL if self.settings.DEBUG else self.settings.ai.GMS_PROD_MODEL
            logger.debug(f"ğŸ¤– Using GMS model: {model}")
            
            data = {
                "model": model,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "messages": [{"role": "user", "content": prompt}]
            }
            
            if system_message:
                data["system"] = system_message
            
            logger.debug(f"ğŸ“ Request data: model={model}, max_tokens={max_tokens}, temperature={temperature}")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.settings.ai.GMS_BASE_URL}/messages",
                    headers=headers,
                    json=data
                )
                response.raise_for_status()
                result = response.json()
                
                # Anthropic API ì‘ë‹µ íŒŒì‹±
                if "content" in result and len(result["content"]) > 0:
                    content = result["content"][0]["text"]
                    logger.info(f"âœ… GMS response received (length: {len(content)} chars)")
                    return content.strip()
                else:
                    logger.error("âŒ Invalid GMS API response format")
                    raise ValueError("Invalid response format from GMS API")
                
        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ GMS API HTTP error {e.response.status_code}: {e.response.text}")
            raise
        except httpx.TimeoutException:
            logger.error("âŒ GMS API timeout")
            raise
        except Exception as e:
            logger.error(f"âŒ GMS completion failed: {e}")
            raise
    
    async def _mock_completion(self, prompt: str) -> str:
        """Generate mock completion for development/testing"""
        logger.debug("Using mock LLM completion")
        
        # Simulate processing time
        await asyncio.sleep(0.5)
        
        # Generate mock response based on prompt content
        if "í€´ì¦ˆ" in prompt or "quiz" in prompt.lower():
            return """1. ì´ ë¬¸ì„œì˜ ì£¼ìš” ì£¼ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?
A) ë…ì„œì˜ ì¤‘ìš”ì„±
B) AI ê¸°ìˆ  ë°œì „
C) êµìœ¡ ë°©ë²•ë¡ 
D) ì†Œì…œ ë„¤íŠ¸ì›Œí‚¹

ì •ë‹µ: A) ë…ì„œì˜ ì¤‘ìš”ì„±
ì„¤ëª…: ë¬¸ì„œ ì „ë°˜ì— ê±¸ì³ ë…ì„œì˜ ì¤‘ìš”ì„±ì´ ê°•ì¡°ë˜ê³  ìˆìŠµë‹ˆë‹¤."""
        
        elif "ì²¨ì‚­" in prompt or "proofread" in prompt.lower():
            return """ìˆ˜ì •ëœ í…ìŠ¤íŠ¸: [ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ê°œì„ ëœ ë²„ì „]
ì£¼ìš” ìˆ˜ì •ì‚¬í•­:
- ë§ì¶¤ë²• êµì •: 2ê°œ
- ë¬¸ë²• êµì •: 1ê°œ  
- ë¬¸ì²´ ê°œì„ : 3ê°œ"""
        
        elif "í† ë¡ " in prompt or "discussion" in prompt.lower():
            return """ì´ ì£¼ì œì— ëŒ€í•´ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ìƒê°í•´ë³¼ ìˆ˜ ìˆê² ë„¤ìš”. 
íŠ¹íˆ [êµ¬ì²´ì  í¬ì¸íŠ¸]ì— ëŒ€í•´ì„œëŠ” ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”? 
ë‹¤ë¥¸ ì°¸ì—¬ìë¶„ë“¤ì˜ ì˜ê²¬ë„ ë“£ê³  ì‹¶ìŠµë‹ˆë‹¤."""
        
        else:
            return f"Mock response for: {prompt[:100]}..."

    async def generate_completion_stream(
        self,
        prompt: str,
        system_message: Optional[str] = None,
        max_tokens: int = 1000,
        temperature: float = 0.7,
        provider: Optional[LLMProvider] = None,
        model: Optional[str] = None
    ):
        """Generate streaming text completion using GMS API (fallback to mock)"""
        
        if self.settings.ai.MOCK_AI_RESPONSES or not self.gms_available:
            async for chunk in self._mock_completion_stream(prompt):
                yield chunk
            return
        
        try:
            # GMS API doesn't support streaming, so we'll return the complete response as a single chunk
            result = await self._gms_completion(prompt, system_message, max_tokens, temperature)
            yield result
        except Exception as e:
            logger.error(f"GMS streaming completion failed: {e}")
            logger.warning("Falling back to mock streaming completion")
            async for chunk in self._mock_completion_stream(prompt):
                yield chunk

    def _get_preferred_provider(self) -> LLMProvider:
        """Get preferred LLM provider based on availability"""
        if self.gms_available:
            return LLMProvider.GMS
        else:
            return LLMProvider.MOCK

    async def _mock_completion_stream(self, prompt: str):
        """Generate mock streaming completion for development/testing"""
        logger.debug("Using mock LLM streaming completion")
        
        # Mock response based on prompt content
        if "í† ë¡ " in prompt or "discussion" in prompt.lower():
            mock_response = """ê·¸ ì˜ê²¬ ì •ë§ í¥ë¯¸ë¡­ë„¤ìš”! íŠ¹íˆ ë§ì”€í•˜ì‹  ë¶€ë¶„ì— ëŒ€í•´ì„œ ë” ìì„¸íˆ ë“¤ì–´ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì°¸ì—¬ìë¶„ë“¤ì€ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?"""
        else:
            mock_response = f"ì´ê²ƒì€ '{prompt[:50]}...'ì— ëŒ€í•œ Mock ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì…ë‹ˆë‹¤."
        
        # Simulate streaming by yielding chunks
        words = mock_response.split()
        for i in range(0, len(words), 2):  # Send 2 words at a time
            chunk = " ".join(words[i:i+2])
            if i > 0:
                chunk = " " + chunk
            yield chunk
            await asyncio.sleep(0.1)  # Simulate network delay
    


# Specialized LLM clients for different use cases
class QuizLLMClient:
    """Specialized client for quiz generation"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
    
    async def generate_quiz(
        self,
        content: str,
        question_count: int = 5,
        difficulty: str = "medium",
        language: str = "ko"
    ) -> List[Dict[str, Any]]:
        """Generate quiz questions from content"""
        
        system_message = f"""ë‹¹ì‹ ì€ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ {difficulty} ë‚œì´ë„ì˜ ê°ê´€ì‹ ë¬¸ì œë¥¼ {question_count}ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
1. ë¬¸ì œ: [ì§ˆë¬¸ ë‚´ìš©]
   A) ì„ íƒì§€1
   B) ì„ íƒì§€2  
   C) ì„ íƒì§€3
   D) ì„ íƒì§€4
   ì •ë‹µ: [A/B/C/D]
   ì„¤ëª…: [ì •ë‹µ ê·¼ê±°]

ë¬¸ì œëŠ” ë‚´ìš©ì˜ í•µì‹¬ì„ ë‹¤ë£¨ê³ , ì´í•´ë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•´ì£¼ì„¸ìš”."""
        
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í€´ì¦ˆë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

{content[:2000]}...

ì–¸ì–´: {language}
ë‚œì´ë„: {difficulty}
ë¬¸ì œ ìˆ˜: {question_count}"""
        
        response = await self.llm_client.generate_completion(
            prompt=prompt,
            system_message=system_message,
            max_tokens=1500,
            temperature=0.3
        )
        
        # Parse response into structured format
        return self._parse_quiz_response(response)
    
    def _parse_quiz_response(self, response: str) -> List[Dict[str, Any]]:
        """Parse LLM response into structured quiz format"""
        questions = []
        
        # Simple parsing logic (could be improved with more robust parsing)
        lines = response.split('\n')
        current_question = None
        options = []
        correct_answer = None
        explanation = ""
        
        for line in lines:
            line = line.strip()
            
            if line.startswith(('1.', '2.', '3.', '4.', '5.')):
                # Save previous question if exists
                if current_question and options:
                    questions.append({
                        "question": current_question,
                        "options": options,
                        "correct_answer": correct_answer,
                        "explanation": explanation
                    })
                
                # Start new question
                current_question = line.split('.', 1)[1].strip()
                options = []
                correct_answer = None
                explanation = ""
            
            elif line.startswith(('A)', 'B)', 'C)', 'D)')):
                option = line[2:].strip()
                options.append(option)
            
            elif line.startswith('ì •ë‹µ:'):
                answer_text = line[3:].strip()
                if answer_text.startswith('A'):
                    correct_answer = 0
                elif answer_text.startswith('B'):
                    correct_answer = 1
                elif answer_text.startswith('C'):
                    correct_answer = 2
                elif answer_text.startswith('D'):
                    correct_answer = 3
            
            elif line.startswith('ì„¤ëª…:'):
                explanation = line[3:].strip()
        
        # Save last question
        if current_question and options:
            questions.append({
                "question": current_question,
                "options": options,
                "correct_answer": correct_answer,
                "explanation": explanation
            })
        
        return questions


class ProofreadingLLMClient:
    """Specialized client for text proofreading"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
    
    async def proofread_text(
        self,
        text: str,
        context: Optional[str] = None,
        language: str = "ko"
    ) -> Dict[str, Any]:
        """Proofread text and provide corrections"""
        
        system_message = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ë²• ë° ë¬¸ì²´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•, ë¬¸ë²•, ë¬¸ì²´ë¥¼ êµì •í•˜ê³  ê°œì„ ì‚¬í•­ì„ ì œì‹œí•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
ìˆ˜ì •ëœ í…ìŠ¤íŠ¸: [êµì •ëœ ì „ì²´ í…ìŠ¤íŠ¸]

ì£¼ìš” ìˆ˜ì •ì‚¬í•­:
1. [ìˆ˜ì • ìœ í˜•]: [ì›ë³¸] â†’ [ìˆ˜ì •ë³¸] (ì´ìœ : [ì„¤ëª…])
2. [ìˆ˜ì • ìœ í˜•]: [ì›ë³¸] â†’ [ìˆ˜ì •ë³¸] (ì´ìœ : [ì„¤ëª…])

ì „ì²´ í‰ê°€: [í…ìŠ¤íŠ¸ í’ˆì§ˆì— ëŒ€í•œ ì¢…í•©ì  í‰ê°€]"""
        
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ êµì •í•´ì£¼ì„¸ìš”:

ì›ë³¸ í…ìŠ¤íŠ¸:
{text}

{"ì»¨í…ìŠ¤íŠ¸: " + context if context else ""}

ì–¸ì–´: {language}"""
        
        response = await self.llm_client.generate_completion(
            prompt=prompt,
            system_message=system_message,
            max_tokens=1000,
            temperature=0.2
        )
        
        return self._parse_proofread_response(response, text)
    
    def _parse_proofread_response(self, response: str, original_text: str) -> Dict[str, Any]:
        """Parse proofreading response"""
        # Simple parsing - could be enhanced
        lines = response.split('\n')
        
        corrected_text = original_text
        corrections = []
        
        for line in lines:
            if line.startswith('ìˆ˜ì •ëœ í…ìŠ¤íŠ¸:'):
                corrected_text = line[7:].strip()
            elif 'â†’' in line:
                # Extract correction
                parts = line.split('â†’')
                if len(parts) == 2:
                    original = parts[0].strip()
                    corrected = parts[1].split('(')[0].strip()
                    reason = ""
                    if 'ì´ìœ :' in line:
                        reason = line.split('ì´ìœ :')[1].strip().rstrip(')')
                    
                    corrections.append({
                        "original": original,
                        "corrected": corrected,
                        "reason": reason,
                        "type": "grammar"  # Could be more specific
                    })
        
        return {
            "corrected_text": corrected_text,
            "corrections": corrections,
            "confidence_score": 0.85  # Mock confidence score
        }


class DiscussionLLMClient:
    """Specialized client for discussion facilitation"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
    
    async def generate_moderator_response(
        self,
        message: str,
        context: List[str],
        conversation_history: List[Dict[str, str]]
    ) -> str:
        """Generate AI moderator response for discussion"""
        
        system_message = """ë‹¹ì‹ ì€ ë…ì„œ í† ë¡ ì„ ì§„í–‰í•˜ëŠ” AI ì‚¬íšŒìì…ë‹ˆë‹¤. 
ì°¸ì—¬ìë“¤ì˜ í† ë¡ ì„ í™œì„±í™”í•˜ê³ , ê¹Šì´ ìˆëŠ” ëŒ€í™”ë¡œ ì´ëŒì–´ì£¼ì„¸ìš”.

ì—­í• :
- í† ë¡  ì£¼ì œì—ì„œ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ê°€ì´ë“œ
- ì°¸ì—¬ê°€ ì ì€ ì‚¬ëŒë“¤ì˜ ë°œì–¸ ìœ ë„
- ë‹¤ì–‘í•œ ê´€ì  ì œì‹œ
- ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê·¼ê±° ì œì‹œ
- ê±´ì„¤ì ì¸ í† ë¡  ë¶„ìœ„ê¸° ì¡°ì„±

ì‘ë‹µì€ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ, 100ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
        
        context_str = "\n".join(context[:3]) if context else ""
        history_str = "\n".join([f"{msg.get('sender', '')}: {msg.get('message', '')}" 
                                for msg in conversation_history[-5:]])
        
        prompt = f"""í˜„ì¬ ë©”ì‹œì§€: {message}

ë¬¸ì„œ ê´€ë ¨ ë‚´ìš©:
{context_str}

ìµœê·¼ ëŒ€í™” ë‚´ìš©:
{history_str}

ìœ„ ìƒí™©ì—ì„œ í† ë¡ ì„ ë” í™œì„±í™”í•  ìˆ˜ ìˆëŠ” AI ì‚¬íšŒìì˜ ì‘ë‹µì„ ìƒì„±í•´ì£¼ì„¸ìš”."""
        
        response = await self.llm_client.generate_completion(
            prompt=prompt,
            system_message=system_message,
            max_tokens=200,
            temperature=0.8
        )
        
        return response.strip()
    
    async def generate_discussion_topics(
        self,
        document_content: str,
        previous_topics: List[str] = None
    ) -> List[str]:
        """Generate discussion topics from document content"""
        
        system_message = """ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ í¥ë¯¸ë¡­ê³  ìƒê°í•´ë³¼ ë§Œí•œ í† ë¡  ì£¼ì œë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.
ì£¼ì œëŠ” ì°¸ì—¬ìë“¤ì´ ë‹¤ì–‘í•œ ì˜ê²¬ì„ ë‚˜ëˆŒ ìˆ˜ ìˆë„ë¡ ê°œë°©í˜•ìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”."""
        
        previous_str = f"\nê¸°ì¡´ ë…¼ì˜ëœ ì£¼ì œë“¤ (í”¼í•´ì£¼ì„¸ìš”):\n{previous_topics}" if previous_topics else ""
        
        prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ í† ë¡  ì£¼ì œ 3ê°œë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”:

{document_content[:1500]}...

{previous_str}

í˜•ì‹:
1. [í† ë¡  ì£¼ì œ]
2. [í† ë¡  ì£¼ì œ]  
3. [í† ë¡  ì£¼ì œ]"""
        
        response = await self.llm_client.generate_completion(
            prompt=prompt,
            system_message=system_message,
            max_tokens=300,
            temperature=0.7
        )
        
        # Parse topics
        topics = []
        for line in response.split('\n'):
            if line.strip() and any(line.startswith(f"{i}.") for i in range(1, 6)):
                topic = line.split('.', 1)[1].strip()
                topics.append(topic)
        
        return topics