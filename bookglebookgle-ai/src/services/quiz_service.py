"""
Quiz Generation Service for BGBG AI Server
Handles quiz creation from document content with progress-based triggers
"""

import asyncio
import json
import re
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any

from loguru import logger

from src.services.llm_client import LLMClient, QuizLLMClient
from src.services.vector_db import VectorDBManager
from src.config.settings import get_settings


class QuizService:
    """Service for generating quizzes from document content"""
    
    def __init__(self):
        self.settings = get_settings()
        self.llm_client: Optional[LLMClient] = None
        self.quiz_llm_client: Optional[QuizLLMClient] = None
        self.vector_db: Optional[VectorDBManager] = None
        self.active_quizzes: Dict[str, Dict[str, Any]] = {}
    
    async def initialize(self):
        """Initialize quiz service dependencies"""
        try:
            logger.info("Initializing Quiz Service...")
            
            # Initialize LLM client only if not already set by ServiceInitializer
            if self.llm_client is None:
                logger.info("üîß Creating new LLM client for Quiz Service")
                self.llm_client = LLMClient()
                await self.llm_client.initialize()
                self.quiz_llm_client = QuizLLMClient(self.llm_client)
            else:
                logger.info("üîÑ Using existing LLM client for Quiz Service")
                # quiz_llm_clientÎäî ServiceInitializerÏóêÏÑú Ïù¥ÎØ∏ ÏÑ§Ï†ïÎê®
            
            # Note: VectorDB is managed by ServiceInitializer, not initialized here
            # self.vector_db is already set by ServiceInitializer - do not reset to None!
            
            logger.info("Quiz Service initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize Quiz Service: {e}")
            raise
    
    async def generate_quiz(self, quiz_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate quiz questions from document content using VectorDB
        
        Args:
            quiz_data: Dict containing:
                - document_id: str
                - meeting_id: str
                - progress_percentage: int (50 or 100)
        
        Returns:
            Dict with success status, quiz_id, and questions
        """
        try:
            logger.info(f"Generating quiz for document: {quiz_data.get('document_id')}")
            
            # DEBUG: VectorDB ÏÉÅÌÉú ÌôïÏù∏
            logger.info(f"üîç DEBUG - VectorDB status: {'Available' if self.vector_db else 'None'}")
            if self.vector_db:
                logger.info(f"üîç DEBUG - VectorDB type: {type(self.vector_db)}")
            
            # Validate input
            if not self._validate_quiz_request(quiz_data):
                return {"success": False, "error": "Invalid quiz request data"}
            
            # VectorDBÏóêÏÑú ÏßÑÎèÑÏú®Î≥Ñ Î¨∏ÏÑú ÎÇ¥Ïö© Í≤ÄÏÉâ (fallback Ìè¨Ìï®)
            combined_content = ""
            if self.vector_db:
                try:
                    content_chunks = await self.vector_db.search_by_progress(
                        meeting_id=quiz_data["meeting_id"],
                        document_id=quiz_data["document_id"],
                        progress_percentage=quiz_data["progress_percentage"],
                        max_chunks=3
                    )
                    
                    if content_chunks:
                        combined_content = "\n\n".join(content_chunks)
                        logger.info(f"Retrieved {len(content_chunks)} chunks from VectorDB")
                    else:
                        logger.warning(f"No content found in VectorDB for document {quiz_data['document_id']} at {quiz_data['progress_percentage']}% progress")
                except Exception as e:
                    logger.warning(f"VectorDB search failed: {e}")
            else:
                logger.warning("VectorDB not available, using fallback content")
            
            # Fallback: VectorDBÏóêÏÑú ÎÇ¥Ïö©ÏùÑ Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏùÑ Îïå Í∏∞Î≥∏ ÎÇ¥Ïö© ÏÇ¨Ïö©
            if not combined_content:
                progress = quiz_data["progress_percentage"]
                if progress == 50:
                    combined_content = f"Î¨∏ÏÑú Ï†ÑÎ∞òÎ∂Ä(50% ÏßÑÎèÑ) ÎÇ¥Ïö©: Í∏∞Î≥∏ Í∞úÎÖêÍ≥º ÎèÑÏûÖÎ∂Ä ÏÑ§Î™ÖÏù¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏäµÎãàÎã§. Ï£ºÏöî Ïö©Ïñ¥ÏôÄ Í∏∞Ï¥à Ïù¥Î°†ÏùÑ Îã§Î£®Î©∞, Ïù¥Ìï¥ÌïòÍ∏∞ Ïâ¨Ïö¥ ÏòàÏãúÎì§Î°ú Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÏäµÎãàÎã§."
                else:  # 100%
                    combined_content = f"Î¨∏ÏÑú Ï†ÑÏ≤¥(100% ÏßÑÎèÑ) ÎÇ¥Ïö©: Í∏∞Î≥∏ Í∞úÎÖêÎ∂ÄÌÑ∞ Ïã¨Ìôî ÎÇ¥Ïö©ÍπåÏßÄ Ìè¨Í¥ÑÏ†ÅÏúºÎ°ú Îã§Î£πÎãàÎã§. Ïù¥Î°†Ï†Å Î∞∞Í≤Ω, Ïã§Î¨¥ Ï†ÅÏö© ÏÇ¨Î°Ä, Í≤∞Î°† Î∞è ÏöîÏïΩÏù¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏäµÎãàÎã§."
                logger.info(f"Using fallback content for {progress}% progress")
            
            quiz_data_with_content = {
                **quiz_data,
                "content": combined_content,
                "language": "ko",
                "question_count": 4,
                "difficulty_level": "medium"
            }
            
            # Ïã§Ï†ú LLM Ïó∞Îèô ÎòêÎäî Mock ÏÑ†ÌÉù
            if not self.settings.ai.MOCK_AI_RESPONSES and self.quiz_llm_client:
                # Ïã§Ï†ú LLMÏùÑ ÏÇ¨Ïö©Ìïú ÌÄ¥Ï¶à ÏÉùÏÑ±
                llm_questions = await self._generate_llm_questions(quiz_data_with_content)
                if llm_questions:
                    mock_questions = llm_questions
                else:
                    logger.warning("LLM quiz generation failed, falling back to mock")
                    mock_questions = self._generate_mock_questions(quiz_data_with_content)
            else:
                # Mock ÏùëÎãµ ÏÇ¨Ïö©
                mock_questions = self._generate_mock_questions(quiz_data_with_content)
            
            # Validate generated questions
            validated_questions = self._validate_questions(mock_questions)
            
            if not validated_questions:
                return {"success": False, "error": "Failed to generate valid questions"}
            
            # Create quiz record
            quiz_id = self._generate_quiz_id(quiz_data["document_id"])
            quiz_record = {
                "quiz_id": quiz_id,
                "document_id": quiz_data["document_id"],
                "meeting_id": quiz_data["meeting_id"],
                "progress_percentage": quiz_data["progress_percentage"],
                "questions": validated_questions,
                "created_at": datetime.utcnow().isoformat(),
                "difficulty_level": "medium",
                "language": "ko",
                "question_count": 4
            }
            
            # Store quiz for future reference
            self.active_quizzes[quiz_id] = quiz_record
            
            logger.info(f"Successfully generated quiz with {len(validated_questions)} questions")
            
            return {
                "success": True,
                "quiz_id": quiz_id,
                "questions": validated_questions,
                "message": f"Quiz generated with {len(validated_questions)} questions"
            }
            
        except Exception as e:
            logger.error(f"Quiz generation failed: {e}")
            return {"success": False, "error": f"Quiz generation failed: {str(e)}"}
    
    async def _extract_content_by_progress(self, quiz_data: Dict[str, Any]) -> str:
        """Extract relevant content based on progress percentage"""
        try:
            content = quiz_data.get("content", "")
            progress_percentage = quiz_data.get("progress_percentage", 100)
            
            if not content:
                return ""
            
            # Split content based on progress
            if progress_percentage == 50:
                # First half of the document
                mid_point = len(content) // 2
                return content[:mid_point]
            elif progress_percentage == 100:
                # Second half of the document  
                mid_point = len(content) // 2
                return content[mid_point:]
            else:
                # Full document for other percentages
                return content
                
        except Exception as e:
            logger.error(f"Content extraction failed: {e}")
            return quiz_data.get("content", "")
    
    def _validate_quiz_request(self, quiz_data: Dict[str, Any]) -> bool:
        """Validate quiz generation request"""
        required_fields = ["document_id", "meeting_id", "progress_percentage"]
        
        for field in required_fields:
            if field not in quiz_data or not quiz_data[field]:
                logger.error(f"Missing required field: {field}")
                return False
        
        # Validate progress percentage
        progress = quiz_data.get("progress_percentage")
        if progress not in [50, 100]:
            logger.error(f"Invalid progress percentage: {progress}. Must be 50 or 100")
            return False
        
        return True
    
    def _generate_mock_questions(self, quiz_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate mock quiz questions for testing"""
        content = quiz_data.get("content", "")
        progress = quiz_data.get("progress_percentage", 100)
        question_count = 4  # Í≥†Ï†ïÍ∞í
        
        # Mock questions based on progress percentage
        if progress == 50:
            mock_questions = [
                {
                    "question": "Î¨∏ÏÑú Ï†ÑÎ∞òÎ∂Ä(50% ÏßÑÎèÑ)Ïùò Ï£ºÏöî ÎÇ¥Ïö©ÏùÄ Î¨¥ÏóáÏûÖÎãàÍπå?",
                    "options": ["Í∏∞Î≥∏ Í∞úÎÖê ÏÜåÍ∞ú", "Ïã¨Ìôî ÎÇ¥Ïö©", "Í≤∞Î°† Î∞è ÏöîÏïΩ", "Ï∞∏Í≥† ÏûêÎ£å"],
                    "correct_answer": 0,
                    "explanation": "Î¨∏ÏÑú Ï†ÑÎ∞òÎ∂ÄÎäî Ï£ºÎ°ú Í∏∞Î≥∏ Í∞úÎÖêÏùÑ ÏÜåÍ∞úÌïòÎäî ÎÇ¥Ïö©ÏûÖÎãàÎã§.",
                    "category": "Ï†ÑÎ∞òÎ∂Ä ÎÇ¥Ïö©"
                },
                {
                    "question": "Î¨∏ÏÑúÏóêÏÑú Ï≤òÏùå Îì±Ïû•ÌïòÎäî ÌïµÏã¨ Ïö©Ïñ¥Îäî?",
                    "options": ["Í∏∞Î≥∏ Ïö©Ïñ¥", "Ï†ÑÎ¨∏ Ïö©Ïñ¥", "Í≥†Í∏â Ïö©Ïñ¥", "Í≤∞Î°† Ïö©Ïñ¥"],
                    "correct_answer": 0,
                    "explanation": "Î¨∏ÏÑú Ï¥àÎ∞òÎ∂ÄÏóêÎäî Í∏∞Î≥∏Ï†ÅÏù∏ Ïö©Ïñ¥Îì§Ïù¥ Ï£ºÎ°ú Îì±Ïû•Ìï©ÎãàÎã§.",
                    "category": "Ïö©Ïñ¥ Ïù¥Ìï¥"
                },
                {
                    "question": "Î¨∏ÏÑú Ï†ÑÎ∞òÎ∂ÄÏóêÏÑú Í∞ïÏ°∞ÌïòÎäî Ï£ºÏöî Ìè¨Ïù∏Ìä∏Îäî?",
                    "options": ["Í∏∞Ï¥à Ïù¥Ìï¥", "Ïã§Î¨¥ Ï†ÅÏö©", "Í≥†Í∏â Í∏∞Î≤ï", "ÏµúÏ¢Ö ÌèâÍ∞Ä"],
                    "correct_answer": 0,
                    "explanation": "Ï†ÑÎ∞òÎ∂ÄÏóêÏÑúÎäî Í∏∞Ï¥àÏ†ÅÏù∏ Ïù¥Ìï¥Î•º Í∞ïÏ°∞Ìï©ÎãàÎã§.",
                    "category": "ÌïôÏäµ Î™©Ìëú"
                },
                {
                    "question": "Î¨∏ÏÑú Ï†ÑÎ∞òÎ∂ÄÏùò Íµ¨ÏÑ± Î∞©ÏãùÏùÄ?",
                    "options": ["Îã®Í≥ÑÎ≥Ñ ÏÑ§Î™Ö", "Î¨¥ÏûëÏúÑ Î∞∞Ïπò", "Í≤∞Î°† Ïö∞ÏÑ†", "Ï∞∏Í≥†ÏûêÎ£å ÏúÑÏ£º"],
                    "correct_answer": 0,
                    "explanation": "Ï†ÑÎ∞òÎ∂ÄÎäî Îã®Í≥ÑÏ†ÅÏúºÎ°ú ÎÇ¥Ïö©ÏùÑ ÏÑ§Î™ÖÌïòÎäî Íµ¨ÏÑ±ÏûÖÎãàÎã§.",
                    "category": "Íµ¨ÏÑ± Î∞©Ïãù"
                }
            ]
        else:  # progress == 100
            mock_questions = [
                {
                    "question": "Ï†ÑÏ≤¥ Î¨∏ÏÑú(100% ÏßÑÎèÑ)Ïùò ÌïµÏã¨ Î©îÏãúÏßÄÎäî?",
                    "options": ["Ï†ÑÏ≤¥Ï†Å Ïù¥Ìï¥", "Î∂ÄÎ∂ÑÏ†Å ÏßÄÏãù", "Í∏∞Ï¥à Í∞úÎÖê", "ÏÑ∏Î∂Ä ÏÇ¨Ìï≠"],
                    "correct_answer": 0,
                    "explanation": "Ï†ÑÏ≤¥ Î¨∏ÏÑúÎ•º ÌÜµÌï¥ Ìè¨Í¥ÑÏ†ÅÏù∏ Ïù¥Ìï¥Î•º Î™©ÌëúÎ°ú Ìï©ÎãàÎã§.",
                    "category": "Ï†ÑÏ≤¥ ÏöîÏïΩ"
                },
                {
                    "question": "Î¨∏ÏÑú ÌõÑÎ∞òÎ∂ÄÏóêÏÑú Îã§Î£®Îäî Í≥†Í∏â ÎÇ¥Ïö©ÏùÄ?",
                    "options": ["Ïã¨Ìôî Í∞úÎÖê", "Í∏∞Î≥∏ Í∞úÎÖê", "ÎèÑÏûÖÎ∂Ä", "Î™©Ï∞®"],
                    "correct_answer": 0,
                    "explanation": "ÌõÑÎ∞òÎ∂ÄÏóêÎäî Ïã¨ÌôîÎêú Í∞úÎÖêÎì§Ïù¥ Ï£ºÎ°ú Îã§Î§ÑÏßëÎãàÎã§.",
                    "category": "ÌõÑÎ∞òÎ∂Ä ÎÇ¥Ïö©"
                },
                {
                    "question": "Î¨∏ÏÑúÏùò Í≤∞Î°† Î∂ÄÎ∂ÑÏóêÏÑú Ï†úÏãúÌïòÎäî Í≤ÉÏùÄ?",
                    "options": ["Ï¢ÖÌï©Ï†Å Ï†ïÎ¶¨", "ÏÉàÎ°úÏö¥ ÏãúÏûë", "Í∏∞Ï¥à ÏÑ§Î™Ö", "Ïö©Ïñ¥ Ï†ïÏùò"],
                    "correct_answer": 0,
                    "explanation": "Í≤∞Î°† Î∂ÄÎ∂ÑÏóêÏÑúÎäî Ï†ÑÏ≤¥ ÎÇ¥Ïö©ÏùÑ Ï¢ÖÌï©Ï†ÅÏúºÎ°ú Ï†ïÎ¶¨Ìï©ÎãàÎã§.",
                    "category": "Í≤∞Î°†"
                },
                {
                    "question": "Ï†ÑÏ≤¥ Î¨∏ÏÑúÎ•º ÌÜµÌï¥ ÏñªÏùÑ Ïàò ÏûàÎäî ÏµúÏ¢Ö Ïù¥Ìï¥Îäî?",
                    "options": ["ÏôÑÏ†ÑÌïú Ïù¥Ìï¥", "Î∂ÄÎ∂ÑÏ†Å ÏßÄÏãù", "Í∏∞Ï¥à ÏàòÏ§Ä", "ÏûÖÎ¨∏ ÏàòÏ§Ä"],
                    "correct_answer": 0,
                    "explanation": "Ï†ÑÏ≤¥ Î¨∏ÏÑúÎ•º ÌÜµÌï¥ Ï£ºÏ†úÏóê ÎåÄÌïú ÏôÑÏ†ÑÌïú Ïù¥Ìï¥Î•º ÏñªÏùÑ Ïàò ÏûàÏäµÎãàÎã§.",
                    "category": "ÏµúÏ¢Ö Î™©Ìëú"
                }
            ]
        
        # Return fixed 4 questions
        return mock_questions[:4]
    
    def _clean_json_response(self, response: str) -> str:
        """LLM ÏùëÎãµÏóêÏÑú JSONÏùÑ Ï†ïÎ¶¨ÌïòÍ≥† ÏàòÏ†ï"""
        # ÎßàÌÅ¨Îã§Ïö¥ ÏΩîÎìú Î∏îÎ°ù Ï†úÍ±∞
        response = re.sub(r'```json\s*|\s*```', '', response)
        
        # Î∂àÌïÑÏöîÌïú ÌÖçÏä§Ìä∏ Ï†úÍ±∞
        response = re.sub(r'^[^\[\{]*', '', response)
        response = re.sub(r'[^}\]]*$', '', response)
        
        # ÏùºÎ∞òÏ†ÅÏù∏ JSON Ïò§Î•ò ÏàòÏ†ï
        response = re.sub(r',(\s*[}\]])', r'\1', response)  # ÎßàÏßÄÎßâ ÏâºÌëú Ï†úÍ±∞
        response = re.sub(r'(["\w])\s*\n\s*(["\w])', r'\1, \2', response)  # ÎàÑÎùΩÎêú ÏâºÌëú Ï∂îÍ∞Ä
        
        return response.strip()
    
    def _fix_json_object(self, obj_str: str) -> str:
        """Í∞úÎ≥Ñ JSON Í∞ùÏ≤¥Ïùò ÏùºÎ∞òÏ†ÅÏù∏ Ïò§Î•ò ÏàòÏ†ï"""
        # ÎàÑÎùΩÎêú ÏâºÌëú Ï∂îÍ∞Ä (Î∞∞Ïó¥ ÎÇ¥)
        obj_str = re.sub(r'"\s*"', '", "', obj_str)
        obj_str = re.sub(r'"\s*]', '"]', obj_str)
        
        # ÎàÑÎùΩÎêú ÏâºÌëú Ï∂îÍ∞Ä (Í∞ùÏ≤¥ ÏÜçÏÑ± Í∞Ñ)
        obj_str = re.sub(r'"\s*"([a-zA-Z_])', r'", "\1', obj_str)
        obj_str = re.sub(r'(\d+)\s*"', r'\1, "', obj_str)
        
        return obj_str
    
    def _is_valid_question_structure(self, q: Dict) -> bool:
        """ÏßàÎ¨∏ Íµ¨Ï°∞Í∞Ä Ïú†Ìö®ÌïúÏßÄ Í≤ÄÏ¶ù"""
        required_keys = ["question", "options", "correct_answer"]
        
        if not all(key in q for key in required_keys):
            return False
        
        options = q.get("options", [])
        if not isinstance(options, list) or len(options) < 2:
            return False
        
        correct_answer = q.get("correct_answer")
        try:
            correct_idx = int(correct_answer)
            if correct_idx < 0 or correct_idx >= len(options):
                return False
        except (ValueError, TypeError):
            return False
        
        return True
    
    def _process_llm_questions(self, questions: List[Dict]) -> List[Dict[str, Any]]:
        """LLMÏù¥ ÏÉùÏÑ±Ìïú ÏßàÎ¨∏Îì§ÏùÑ ÌëúÏ§Ä ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò"""
        formatted_questions = []
        
        for q in questions:
            if isinstance(q, dict) and self._is_valid_question_structure(q):
                formatted_questions.append({
                    "question": str(q.get("question", "")).strip(),
                    "options": [str(opt).strip() for opt in q.get("options", [])],
                    "correct_answer": int(q.get("correct_answer", 0)),
                    "explanation": str(q.get("explanation", "")).strip(),
                    "category": "LLM ÏÉùÏÑ±"
                })
        
        return formatted_questions
    
    async def _generate_llm_questions(self, quiz_data: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:
        """LLMÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ïã§Ï†ú ÌÄ¥Ï¶à ÏÉùÏÑ±"""
        try:
            content = quiz_data.get("content", "")
            language = quiz_data.get("language", "ko")
            question_count = quiz_data.get("question_count", 5)
            difficulty = quiz_data.get("difficulty_level", "medium")
            progress = quiz_data.get("progress_percentage", 100)
            
            if not content:
                logger.error("No content provided for LLM quiz generation")
                return None
            
            # ÏãúÏä§ÌÖú Î©îÏãúÏßÄÏôÄ ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
            system_message = """ÎãπÏã†ÏùÄ ÍµêÏú° Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. Ï£ºÏñ¥ÏßÑ Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Î∞îÌÉïÏúºÎ°ú Í∞ùÍ¥ÄÏãù ÌÄ¥Ï¶àÎ•º ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.

CRITICAL: Î∞òÎìúÏãú Ïú†Ìö®Ìïú JSON Î∞∞Ïó¥ ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµÌïòÏÑ∏Ïöî. Îã§Î•∏ ÌÖçÏä§Ìä∏Îäî Ìè¨Ìï®ÌïòÏßÄ ÎßàÏÑ∏Ïöî.

Ï†ïÌôïÌïú ÌòïÏãù:
[
  {
    "question": "ÏßàÎ¨∏ ÎÇ¥Ïö©",
    "options": ["ÏÑ†ÌÉùÏßÄ1", "ÏÑ†ÌÉùÏßÄ2", "ÏÑ†ÌÉùÏßÄ3", "ÏÑ†ÌÉùÏßÄ4"],
    "correct_answer": 0,
    "explanation": "Ï†ïÎãµ ÏÑ§Î™Ö"
  }
]

Ï£ºÏùòÏÇ¨Ìï≠:
- Î™®Îì† Î¨∏ÏûêÏó¥ÏùÄ ÏåçÎî∞Ïò¥ÌëúÎ°ú Í∞êÏã∏ÏÑ∏Ïöî
- Î∞∞Ïó¥ ÏöîÏÜå Í∞Ñ ÏâºÌëúÎ•º Îπ†Îú®Î¶¨ÏßÄ ÎßàÏÑ∏Ïöî
- ÎßàÏßÄÎßâ ÏöîÏÜå Îí§ÏóêÎäî ÏâºÌëúÎ•º Î∂ôÏù¥ÏßÄ ÎßàÏÑ∏Ïöî
- Î™®Îì† ÏßàÎ¨∏Í≥º ÏÑ†ÌÉùÏßÄÎäî ÌïúÍµ≠Ïñ¥Î°ú ÏûëÏÑ±ÌïòÏÑ∏Ïöî"""
            
            prompt = f"""Îã§Ïùå Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Î∞îÌÉïÏúºÎ°ú 4Í∞úÏùò Í∞ùÍ¥ÄÏãù ÌÄ¥Ï¶àÎ•º ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.

Î¨∏ÏÑú ÎÇ¥Ïö©:
{content[:2000]}  # ÌÜ†ÌÅ∞ Ï†úÌïúÏùÑ ÏúÑÌï¥ ÎÇ¥Ïö© Ï†úÌïú

ÏßÑÎèÑÏú®: {progress}%
ÎÇúÏù¥ÎèÑ: {difficulty}
Ïñ∏Ïñ¥: {language}

Í∞Å Î¨∏Ï†úÎäî 4Í∞úÏùò ÏÑ†ÌÉùÏßÄÎ•º Í∞ÄÏ†∏Ïïº ÌïòÎ©∞, Ï†ïÎãµÏùÄ 0-3 Ï§ë ÌïòÎÇòÏùò Ïù∏Îç±Ïä§Ïó¨Ïïº Ìï©ÎãàÎã§.
Î∞òÎìúÏãú JSON Î∞∞Ïó¥ ÌòïÌÉúÎ°úÎßå ÏùëÎãµÌïòÏÑ∏Ïöî."""
            
            # LLM ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î•º ÌÜµÌïú ÌÄ¥Ï¶à ÏÉùÏÑ±
            response = await self.llm_client.generate_completion(
                prompt=prompt,
                system_message=system_message,
                max_tokens=2000,
                temperature=0.7,
                provider=LLMProvider.GMS
            )
            
            logger.debug(f"LLM Response: {response[:500]}...")  # ÎîîÎ≤ÑÍπÖÏö© Î°úÍ∑∏
            
            # Í∞ïÌôîÎêú JSON ÌååÏã±
            
            try:
                # 1Îã®Í≥Ñ: ÏùëÎãµ Ï†ïÎ¶¨
                cleaned_response = self._clean_json_response(response)
                logger.debug(f"Cleaned response: {cleaned_response[:300]}...")
                
                # 2Îã®Í≥Ñ: ÏßÅÏ†ë JSON ÌååÏã± ÏãúÎèÑ
                try:
                    questions = json.loads(cleaned_response)
                    if isinstance(questions, list):
                        logger.info("Successfully parsed JSON array directly")
                        return self._process_llm_questions(questions)
                except json.JSONDecodeError as e:
                    logger.debug(f"Direct JSON parsing failed: {e}")
                
                # 3Îã®Í≥Ñ: Î∞∞Ïó¥ Ï∂îÏ∂ú ÏãúÎèÑ
                array_match = re.search(r'\[.*?\]', cleaned_response, re.DOTALL)
                if array_match:
                    try:
                        json_str = array_match.group()
                        questions = json.loads(json_str)
                        if isinstance(questions, list):
                            logger.info("Successfully parsed JSON array from match")
                            return self._process_llm_questions(questions)
                    except json.JSONDecodeError as e:
                        logger.debug(f"Array extraction parsing failed: {e}")
                
                # 4Îã®Í≥Ñ: Í∞úÎ≥Ñ Í∞ùÏ≤¥ Ï∂îÏ∂ú Î∞è Î∞∞Ïó¥ Íµ¨ÏÑ±
                logger.debug("Attempting individual object parsing")
                questions = []
                # Ï§ëÍ¥ÑÌò∏ Í∑†Ìòï ÎßûÏ∂îÍ∏∞ - Îçî Ï†ïÌôïÌïú Ï†ïÍ∑úÏãù
                objects = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned_response)
                
                for i, obj_str in enumerate(objects):
                    try:
                        # Í∞úÎ≥Ñ Í∞ùÏ≤¥ Ï†ïÎ¶¨
                        fixed_obj = self._fix_json_object(obj_str)
                        question = json.loads(fixed_obj)
                        questions.append(question)
                        logger.debug(f"Successfully parsed object {i+1}")
                    except json.JSONDecodeError as e:
                        logger.warning(f"Failed to parse individual JSON object {i+1}: {e}")
                        logger.debug(f"Failed object: {obj_str}")
                        continue
                
                if questions:
                    logger.info(f"Successfully parsed {len(questions)} questions from individual objects")
                    return self._process_llm_questions(questions)
                else:
                    logger.warning("No valid questions found after all parsing attempts")
                    return None
                    
            except Exception as e:
                logger.error(f"JSON parsing completely failed: {e}")
                logger.debug(f"Original response: {response}")
                return None
                
        except Exception as e:
            logger.error(f"LLM quiz generation failed: {e}")
            return None
    
    def _validate_questions(self, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Validate and filter generated questions"""
        validated = []
        
        for i, question in enumerate(questions):
            try:
                # Check required fields
                if not all(key in question for key in ["question", "options", "correct_answer"]):
                    logger.warning(f"Question {i+1} missing required fields")
                    continue
                
                # Validate options
                options = question.get("options", [])
                if not isinstance(options, list) or len(options) < 2:
                    logger.warning(f"Question {i+1} has invalid options")
                    continue
                
                # Validate correct answer
                correct_answer = question.get("correct_answer")
                if not isinstance(correct_answer, int) or correct_answer < 0 or correct_answer >= len(options):
                    logger.warning(f"Question {i+1} has invalid correct answer")
                    continue
                
                # Clean up question text
                clean_question = {
                    "question_text": str(question["question"]).strip(),
                    "options": [str(opt).strip() for opt in options],
                    "correct_answer_index": int(correct_answer),
                    "explanation": str(question.get("explanation", "")).strip(),
                    "category": question.get("category", "general")
                }
                
                validated.append(clean_question)
                
            except Exception as e:
                logger.warning(f"Failed to validate question {i+1}: {e}")
                continue
        
        logger.info(f"Validated {len(validated)} out of {len(questions)} questions")
        return validated
    
    def _generate_quiz_id(self, document_id: str) -> str:
        """Generate unique quiz ID"""
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        return f"quiz_{document_id}_{timestamp}_{unique_id}"
    
    async def get_quiz_by_id(self, quiz_id: str) -> Optional[Dict[str, Any]]:
        """Retrieve quiz by ID"""
        return self.active_quizzes.get(quiz_id)
    
    async def submit_quiz_answers(
        self,
        quiz_id: str,
        user_id: str,
        answers: List[int]
    ) -> Dict[str, Any]:
        """
        Submit and evaluate quiz answers
        
        Args:
            quiz_id: Quiz identifier
            user_id: User identifier
            answers: List of selected answer indices
        
        Returns:
            Dict with evaluation results
        """
        try:
            logger.info(f"Processing quiz submission for user {user_id}")
            
            quiz = self.active_quizzes.get(quiz_id)
            if not quiz:
                return {"success": False, "error": "Quiz not found"}
            
            questions = quiz.get("questions", [])
            if len(answers) != len(questions):
                return {"success": False, "error": "Answer count mismatch"}
            
            # Evaluate answers
            results = []
            correct_count = 0
            
            for i, (question, user_answer) in enumerate(zip(questions, answers)):
                correct_answer = question["correct_answer_index"]
                is_correct = user_answer == correct_answer
                
                if is_correct:
                    correct_count += 1
                
                results.append({
                    "question_index": i,
                    "question": question["question_text"],
                    "user_answer": user_answer,
                    "correct_answer": correct_answer,
                    "is_correct": is_correct,
                    "explanation": question.get("explanation", "")
                })
            
            # Calculate score
            score = (correct_count / len(questions)) * 100
            
            # Store submission record
            submission_record = {
                "quiz_id": quiz_id,
                "user_id": user_id,
                "answers": answers,
                "results": results,
                "score": score,
                "submitted_at": datetime.utcnow().isoformat()
            }
            
            logger.info(f"Quiz submission evaluated: {correct_count}/{len(questions)} correct ({score:.1f}%)")
            
            return {
                "success": True,
                "quiz_id": quiz_id,
                "score": score,
                "correct_count": correct_count,
                "total_questions": len(questions),
                "results": results,
                "message": f"Quiz completed with score {score:.1f}%"
            }
            
        except Exception as e:
            logger.error(f"Quiz submission processing failed: {e}")
            return {"success": False, "error": f"Submission processing failed: {str(e)}"}
    
    async def get_quiz_statistics(self, document_id: str) -> Dict[str, Any]:
        """Get quiz statistics for a document"""
        try:
            # Filter quizzes for the document
            document_quizzes = [
                quiz for quiz in self.active_quizzes.values()
                if quiz.get("document_id") == document_id
            ]
            
            if not document_quizzes:
                return {"success": False, "error": "No quizzes found for document"}
            
            stats = {
                "document_id": document_id,
                "total_quizzes": len(document_quizzes),
                "quiz_types": {
                    "50_percent": len([q for q in document_quizzes if q.get("progress_percentage") == 50]),
                    "100_percent": len([q for q in document_quizzes if q.get("progress_percentage") == 100])
                },
                "difficulty_distribution": {},
                "recent_quizzes": []
            }
            
            # Difficulty distribution
            for quiz in document_quizzes:
                difficulty = quiz.get("difficulty_level", "medium")
                stats["difficulty_distribution"][difficulty] = \
                    stats["difficulty_distribution"].get(difficulty, 0) + 1
            
            # Recent quizzes (last 5)
            sorted_quizzes = sorted(
                document_quizzes,
                key=lambda q: q.get("created_at", ""),
                reverse=True
            )
            
            stats["recent_quizzes"] = [
                {
                    "quiz_id": quiz["quiz_id"],
                    "created_at": quiz.get("created_at"),
                    "question_count": len(quiz.get("questions", [])),
                    "progress_percentage": quiz.get("progress_percentage"),
                    "difficulty_level": quiz.get("difficulty_level")
                }
                for quiz in sorted_quizzes[:5]
            ]
            
            return {"success": True, **stats}
            
        except Exception as e:
            logger.error(f"Failed to get quiz statistics: {e}")
            return {"success": False, "error": str(e)}
    
    async def cleanup_old_quizzes(self, max_age_hours: int = 24):
        """Clean up old quiz records"""
        try:
            current_time = datetime.utcnow()
            cleaned_count = 0
            
            quiz_ids_to_remove = []
            
            for quiz_id, quiz in self.active_quizzes.items():
                created_at_str = quiz.get("created_at")
                if created_at_str:
                    created_at = datetime.fromisoformat(created_at_str)
                    age_hours = (current_time - created_at).total_seconds() / 3600
                    
                    if age_hours > max_age_hours:
                        quiz_ids_to_remove.append(quiz_id)
            
            # Remove old quizzes
            for quiz_id in quiz_ids_to_remove:
                del self.active_quizzes[quiz_id]
                cleaned_count += 1
            
            if cleaned_count > 0:
                logger.info(f"Cleaned up {cleaned_count} old quizzes")
            
        except Exception as e:
            logger.error(f"Quiz cleanup failed: {e}")