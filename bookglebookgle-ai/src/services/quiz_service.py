"""
Quiz Generation Service for BGBG AI Server
Handles quiz creation from document content with progress-based triggers
"""

import asyncio
import json
import re
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any

from loguru import logger

from src.services.llm_client import LLMClient, QuizLLMClient, LLMProvider
from src.services.vector_db import VectorDBManager
from src.config.settings import get_settings


class QuizService:
    """Service for generating quizzes from document content"""
    
    def __init__(self):
        self.settings = get_settings()
        self.llm_client: Optional[LLMClient] = None
        self.quiz_llm_client: Optional[QuizLLMClient] = None
        self.vector_db: Optional[VectorDBManager] = None
        self.active_quizzes: Dict[str, Dict[str, Any]] = {}
    
    async def initialize(self):
        """Initialize quiz service dependencies"""
        try:
            logger.info("Initializing Quiz Service...")
            
            # Initialize LLM client only if not already set by ServiceInitializer
            if self.llm_client is None:
                logger.info("ğŸ”§ Creating new LLM client for Quiz Service")
                self.llm_client = LLMClient()
                await self.llm_client.initialize()
                self.quiz_llm_client = QuizLLMClient(self.llm_client)
            else:
                logger.info("ğŸ”„ Using existing LLM client for Quiz Service")
                # quiz_llm_clientëŠ” ServiceInitializerì—ì„œ ì´ë¯¸ ì„¤ì •ë¨
            
            # Note: VectorDB is managed by ServiceInitializer, not initialized here
            # self.vector_db is already set by ServiceInitializer - do not reset to None!
            
            logger.info("Quiz Service initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize Quiz Service: {e}")
            raise
    
    async def generate_quiz(self, quiz_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate quiz questions from document content using VectorDB
        
        Args:
            quiz_data: Dict containing:
                - document_id: str
                - meeting_id: str
                - progress_percentage: int (50 or 100)
        
        Returns:
            Dict with success status, quiz_id, and questions
        """
        try:
            logger.info(f"Generating quiz for document: {quiz_data.get('document_id')}")
            
            # DEBUG: VectorDB ìƒíƒœ í™•ì¸
            logger.info(f"ğŸ” DEBUG - VectorDB status: {'Available' if self.vector_db else 'None'}")
            if self.vector_db:
                logger.info(f"ğŸ” DEBUG - VectorDB type: {type(self.vector_db)}")
            
            # Validate input
            if not self._validate_quiz_request(quiz_data):
                return {"success": False, "error": "Invalid quiz request data"}
            
            # VectorDBì—ì„œ ì§„ë„ìœ¨ë³„ ë¬¸ì„œ ë‚´ìš© ê²€ìƒ‰ (fallback í¬í•¨)
            combined_content = ""
            if self.vector_db:
                try:
                    content_chunks = await self.vector_db.search_by_progress(
                        meeting_id=quiz_data["meeting_id"],
                        document_id=quiz_data["document_id"],
                        progress_percentage=quiz_data["progress_percentage"],
                        max_chunks=3
                    )
                    
                    if content_chunks:
                        # VectorDBì—ì„œ ì´ë¯¸ ì˜¬ë°”ë¥¸ ì§„ë„ìœ¨ ì²­í¬ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ì¶”ê°€ í•„í„°ë§ ë¶ˆí•„ìš”
                        combined_content = "\n\n".join(content_chunks)
                        logger.info(f"Retrieved {len(content_chunks)} chunks from VectorDB for {quiz_data['progress_percentage']}% progress")
                    else:
                        logger.warning(f"No content found in VectorDB for document {quiz_data['document_id']} at {quiz_data['progress_percentage']}% progress")
                except Exception as e:
                    logger.warning(f"VectorDB search failed: {e}")
            else:
                logger.warning("VectorDB not available, using fallback content")
            
            # Fallback: VectorDBì—ì„œ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì„ ë•Œ ê¸°ë³¸ ë‚´ìš© ì‚¬ìš©
            if not combined_content:
                progress = quiz_data["progress_percentage"]
                if progress == 50:
                    combined_content = f"ë¬¸ì„œ ì „ë°˜ë¶€(50% ì§„ë„) ë‚´ìš©: ê¸°ë³¸ ê°œë…ê³¼ ë„ì…ë¶€ ì„¤ëª…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ìš©ì–´ì™€ ê¸°ì´ˆ ì´ë¡ ì„ ë‹¤ë£¨ë©°, ì´í•´í•˜ê¸° ì‰¬ìš´ ì˜ˆì‹œë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
                else:  # 100%
                    combined_content = f"ë¬¸ì„œ ì „ì²´(100% ì§„ë„) ë‚´ìš©: ê¸°ë³¸ ê°œë…ë¶€í„° ì‹¬í™” ë‚´ìš©ê¹Œì§€ í¬ê´„ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤. ì´ë¡ ì  ë°°ê²½, ì‹¤ë¬´ ì ìš© ì‚¬ë¡€, ê²°ë¡  ë° ìš”ì•½ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
                logger.info(f"Using fallback content for {progress}% progress")
            
            quiz_data_with_content = {
                **quiz_data,
                "content": combined_content,
                "language": "ko",
                "question_count": 4,
                "difficulty_level": "medium"
            }
            
            # ì‹¤ì œ LLM ì—°ë™ ë˜ëŠ” Mock ì„ íƒ
            if not self.settings.ai.MOCK_AI_RESPONSES and self.quiz_llm_client:
                # ì‹¤ì œ LLMì„ ì‚¬ìš©í•œ í€´ì¦ˆ ìƒì„±
                llm_questions = await self._generate_llm_questions(quiz_data_with_content)
                if llm_questions:
                    mock_questions = llm_questions
                else:
                    logger.warning("LLM quiz generation failed, falling back to mock")
                    mock_questions = self._generate_mock_questions(quiz_data_with_content)
            else:
                # Mock ì‘ë‹µ ì‚¬ìš©
                mock_questions = self._generate_mock_questions(quiz_data_with_content)
            
            # Validate generated questions
            validated_questions = self._validate_questions(mock_questions)
            
            if not validated_questions:
                return {"success": False, "error": "Failed to generate valid questions"}
            
            # Create quiz record
            quiz_id = self._generate_quiz_id(quiz_data["document_id"])
            quiz_record = {
                "quiz_id": quiz_id,
                "document_id": quiz_data["document_id"],
                "meeting_id": quiz_data["meeting_id"],
                "progress_percentage": quiz_data["progress_percentage"],
                "questions": validated_questions,
                "created_at": datetime.utcnow().isoformat(),
                "difficulty_level": "medium",
                "language": "ko",
                "question_count": 4
            }
            
            # Store quiz for future reference
            self.active_quizzes[quiz_id] = quiz_record
            
            logger.info(f"Successfully generated quiz with {len(validated_questions)} questions")
            
            return {
                "success": True,
                "quiz_id": quiz_id,
                "questions": validated_questions,
                "message": f"Quiz generated with {len(validated_questions)} questions"
            }
            
        except Exception as e:
            logger.error(f"Quiz generation failed: {e}")
            return {"success": False, "error": f"Quiz generation failed: {str(e)}"}
    
    async def _extract_content_by_progress(self, quiz_data: Dict[str, Any]) -> str:
        """Extract relevant content based on progress percentage"""
        try:
            content = quiz_data.get("content", "")
            progress_percentage = quiz_data.get("progress_percentage", 100)
            
            if not content:
                return ""
            
            # Split content based on progress
            if progress_percentage == 50:
                # First half of the document
                mid_point = len(content) // 2
                return content[:mid_point]
            elif progress_percentage == 100:
                # Second half of the document  
                mid_point = len(content) // 2
                return content[mid_point:]
            else:
                # Full document for other percentages
                return content
                
        except Exception as e:
            logger.error(f"Content extraction failed: {e}")
            return quiz_data.get("content", "")
    
    def _validate_quiz_request(self, quiz_data: Dict[str, Any]) -> bool:
        """Validate quiz generation request"""
        required_fields = ["document_id", "meeting_id", "progress_percentage"]
        
        for field in required_fields:
            if field not in quiz_data or not quiz_data[field]:
                logger.error(f"Missing required field: {field}")
                return False
        
        # Validate progress percentage
        progress = quiz_data.get("progress_percentage")
        if progress not in [50, 100]:
            logger.error(f"Invalid progress percentage: {progress}. Must be 50 or 100")
            return False
        
        return True
    
    def _generate_mock_questions(self, quiz_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate mock quiz questions for testing"""
        content = quiz_data.get("content", "")
        progress = quiz_data.get("progress_percentage", 100)
        question_count = 4  # ê³ ì •ê°’
        
        # Mock questions based on progress percentage
        if progress == 50:
            mock_questions = [
                {
                    "question": "ë¬¸ì„œ ì „ë°˜ë¶€(50% ì§„ë„)ì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?",
                    "options": ["ê¸°ë³¸ ê°œë… ì†Œê°œ", "ì‹¬í™” ë‚´ìš©", "ê²°ë¡  ë° ìš”ì•½", "ì°¸ê³  ìë£Œ"],
                    "correct_answer": 0,
                    "explanation": "ë¬¸ì„œ ì „ë°˜ë¶€ëŠ” ì£¼ë¡œ ê¸°ë³¸ ê°œë…ì„ ì†Œê°œí•˜ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.",
                    "category": "ì „ë°˜ë¶€ ë‚´ìš©"
                },
                {
                    "question": "ë¬¸ì„œì—ì„œ ì²˜ìŒ ë“±ì¥í•˜ëŠ” í•µì‹¬ ìš©ì–´ëŠ”?",
                    "options": ["ê¸°ë³¸ ìš©ì–´", "ì „ë¬¸ ìš©ì–´", "ê³ ê¸‰ ìš©ì–´", "ê²°ë¡  ìš©ì–´"],
                    "correct_answer": 0,
                    "explanation": "ë¬¸ì„œ ì´ˆë°˜ë¶€ì—ëŠ” ê¸°ë³¸ì ì¸ ìš©ì–´ë“¤ì´ ì£¼ë¡œ ë“±ì¥í•©ë‹ˆë‹¤.",
                    "category": "ìš©ì–´ ì´í•´"
                },
                {
                    "question": "ë¬¸ì„œ ì „ë°˜ë¶€ì—ì„œ ê°•ì¡°í•˜ëŠ” ì£¼ìš” í¬ì¸íŠ¸ëŠ”?",
                    "options": ["ê¸°ì´ˆ ì´í•´", "ì‹¤ë¬´ ì ìš©", "ê³ ê¸‰ ê¸°ë²•", "ìµœì¢… í‰ê°€"],
                    "correct_answer": 0,
                    "explanation": "ì „ë°˜ë¶€ì—ì„œëŠ” ê¸°ì´ˆì ì¸ ì´í•´ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.",
                    "category": "í•™ìŠµ ëª©í‘œ"
                },
                {
                    "question": "ë¬¸ì„œ ì „ë°˜ë¶€ì˜ êµ¬ì„± ë°©ì‹ì€?",
                    "options": ["ë‹¨ê³„ë³„ ì„¤ëª…", "ë¬´ì‘ìœ„ ë°°ì¹˜", "ê²°ë¡  ìš°ì„ ", "ì°¸ê³ ìë£Œ ìœ„ì£¼"],
                    "correct_answer": 0,
                    "explanation": "ì „ë°˜ë¶€ëŠ” ë‹¨ê³„ì ìœ¼ë¡œ ë‚´ìš©ì„ ì„¤ëª…í•˜ëŠ” êµ¬ì„±ì…ë‹ˆë‹¤.",
                    "category": "êµ¬ì„± ë°©ì‹"
                }
            ]
        else:  # progress == 100
            mock_questions = [
                {
                    "question": "ì „ì²´ ë¬¸ì„œ(100% ì§„ë„)ì˜ í•µì‹¬ ë©”ì‹œì§€ëŠ”?",
                    "options": ["ì „ì²´ì  ì´í•´", "ë¶€ë¶„ì  ì§€ì‹", "ê¸°ì´ˆ ê°œë…", "ì„¸ë¶€ ì‚¬í•­"],
                    "correct_answer": 0,
                    "explanation": "ì „ì²´ ë¬¸ì„œë¥¼ í†µí•´ í¬ê´„ì ì¸ ì´í•´ë¥¼ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.",
                    "category": "ì „ì²´ ìš”ì•½"
                },
                {
                    "question": "ë¬¸ì„œ í›„ë°˜ë¶€ì—ì„œ ë‹¤ë£¨ëŠ” ê³ ê¸‰ ë‚´ìš©ì€?",
                    "options": ["ì‹¬í™” ê°œë…", "ê¸°ë³¸ ê°œë…", "ë„ì…ë¶€", "ëª©ì°¨"],
                    "correct_answer": 0,
                    "explanation": "í›„ë°˜ë¶€ì—ëŠ” ì‹¬í™”ëœ ê°œë…ë“¤ì´ ì£¼ë¡œ ë‹¤ë¤„ì§‘ë‹ˆë‹¤.",
                    "category": "í›„ë°˜ë¶€ ë‚´ìš©"
                },
                {
                    "question": "ë¬¸ì„œì˜ ê²°ë¡  ë¶€ë¶„ì—ì„œ ì œì‹œí•˜ëŠ” ê²ƒì€?",
                    "options": ["ì¢…í•©ì  ì •ë¦¬", "ìƒˆë¡œìš´ ì‹œì‘", "ê¸°ì´ˆ ì„¤ëª…", "ìš©ì–´ ì •ì˜"],
                    "correct_answer": 0,
                    "explanation": "ê²°ë¡  ë¶€ë¶„ì—ì„œëŠ” ì „ì²´ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.",
                    "category": "ê²°ë¡ "
                },
                {
                    "question": "ì „ì²´ ë¬¸ì„œë¥¼ í†µí•´ ì–»ì„ ìˆ˜ ìˆëŠ” ìµœì¢… ì´í•´ëŠ”?",
                    "options": ["ì™„ì „í•œ ì´í•´", "ë¶€ë¶„ì  ì§€ì‹", "ê¸°ì´ˆ ìˆ˜ì¤€", "ì…ë¬¸ ìˆ˜ì¤€"],
                    "correct_answer": 0,
                    "explanation": "ì „ì²´ ë¬¸ì„œë¥¼ í†µí•´ ì£¼ì œì— ëŒ€í•œ ì™„ì „í•œ ì´í•´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "category": "ìµœì¢… ëª©í‘œ"
                }
            ]
        
        # Return fixed 4 questions
        return mock_questions[:4]
    
    def _clean_json_response(self, response: str) -> str:
        """LLM ì‘ë‹µì—ì„œ JSONì„ ì •ë¦¬í•˜ê³  ìˆ˜ì •"""
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        response = re.sub(r'```json\s*|\s*```', '', response)
        
        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        response = re.sub(r'^[^\[\{]*', '', response)
        response = re.sub(r'[^}\]]*$', '', response)
        
        # ì¼ë°˜ì ì¸ JSON ì˜¤ë¥˜ ìˆ˜ì •
        response = re.sub(r',(\s*[}\]])', r'\1', response)  # ë§ˆì§€ë§‰ ì‰¼í‘œ ì œê±°
        response = re.sub(r'(["\w])\s*\n\s*(["\w])', r'\1, \2', response)  # ëˆ„ë½ëœ ì‰¼í‘œ ì¶”ê°€
        
        return response.strip()
    
    def _fix_json_object(self, obj_str: str) -> str:
        """ê°œë³„ JSON ê°ì²´ì˜ ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ìˆ˜ì •"""
        # ëˆ„ë½ëœ ì‰¼í‘œ ì¶”ê°€ (ë°°ì—´ ë‚´)
        obj_str = re.sub(r'"\s*"', '", "', obj_str)
        obj_str = re.sub(r'"\s*]', '"]', obj_str)
        
        # ëˆ„ë½ëœ ì‰¼í‘œ ì¶”ê°€ (ê°ì²´ ì†ì„± ê°„)
        obj_str = re.sub(r'"\s*"([a-zA-Z_])', r'", "\1', obj_str)
        obj_str = re.sub(r'(\d+)\s*"', r'\1, "', obj_str)
        
        return obj_str
    
    def _is_valid_question_structure(self, q: Dict) -> bool:
        """ì§ˆë¬¸ êµ¬ì¡°ê°€ ìœ íš¨í•œì§€ ê²€ì¦"""
        required_keys = ["question", "options", "correct_answer"]
        
        if not all(key in q for key in required_keys):
            return False
        
        options = q.get("options", [])
        if not isinstance(options, list) or len(options) < 2:
            return False
        
        correct_answer = q.get("correct_answer")
        try:
            correct_idx = int(correct_answer)
            if correct_idx < 0 or correct_idx >= len(options):
                return False
        except (ValueError, TypeError):
            return False
        
        return True
    
    def _process_llm_questions(self, questions: List[Dict]) -> List[Dict[str, Any]]:
        """LLMì´ ìƒì„±í•œ ì§ˆë¬¸ë“¤ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        formatted_questions = []
        
        for q in questions:
            if isinstance(q, dict) and self._is_valid_question_structure(q):
                formatted_questions.append({
                    "question": str(q.get("question", "")).strip(),
                    "options": [str(opt).strip() for opt in q.get("options", [])],
                    "correct_answer": int(q.get("correct_answer", 0)),
                    "explanation": str(q.get("explanation", "")).strip(),
                    "category": "LLM ìƒì„±"
                })
        
        return formatted_questions
    
    async def _generate_llm_questions(self, quiz_data: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ í€´ì¦ˆ ìƒì„±"""
        try:
            content = quiz_data.get("content", "")
            language = quiz_data.get("language", "ko")
            question_count = quiz_data.get("question_count", 5)
            difficulty = quiz_data.get("difficulty_level", "medium")
            progress = quiz_data.get("progress_percentage", 100)
            
            if not content:
                logger.error("No content provided for LLM quiz generation")
                return None
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_message = """ë‹¹ì‹ ì€ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì‹ í€´ì¦ˆë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

CRITICAL: ë°˜ë“œì‹œ ìœ íš¨í•œ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

ì •í™•í•œ í˜•ì‹:
[
  {
    "question": "ì§ˆë¬¸ ë‚´ìš©",
    "options": ["ì„ íƒì§€1", "ì„ íƒì§€2", "ì„ íƒì§€3", "ì„ íƒì§€4"],
    "correct_answer": 0,
    "explanation": "ì •ë‹µ ì„¤ëª…"
  }
]

ì£¼ì˜ì‚¬í•­:
- ëª¨ë“  ë¬¸ìì—´ì€ ìŒë”°ì˜´í‘œë¡œ ê°ì‹¸ì„¸ìš”
- ë°°ì—´ ìš”ì†Œ ê°„ ì‰¼í‘œë¥¼ ë¹ ëœ¨ë¦¬ì§€ ë§ˆì„¸ìš”
- ë§ˆì§€ë§‰ ìš”ì†Œ ë’¤ì—ëŠ” ì‰¼í‘œë¥¼ ë¶™ì´ì§€ ë§ˆì„¸ìš”
- ëª¨ë“  ì§ˆë¬¸ê³¼ ì„ íƒì§€ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”"""
            
            prompt = f"""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ 4ê°œì˜ ê°ê´€ì‹ í€´ì¦ˆë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{content[:2000]}  # í† í° ì œí•œì„ ìœ„í•´ ë‚´ìš© ì œí•œ

ì§„ë„ìœ¨: {progress}%
ë‚œì´ë„: {difficulty}
ì–¸ì–´: {language}

ê° ë¬¸ì œëŠ” 4ê°œì˜ ì„ íƒì§€ë¥¼ ê°€ì ¸ì•¼ í•˜ë©°, ì •ë‹µì€ 0-3 ì¤‘ í•˜ë‚˜ì˜ ì¸ë±ìŠ¤ì—¬ì•¼ í•©ë‹ˆë‹¤.
ë°˜ë“œì‹œ JSON ë°°ì—´ í˜•íƒœë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."""
            
            # LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•œ í€´ì¦ˆ ìƒì„±
            response = await self.llm_client.generate_completion(
                prompt=prompt,
                system_message=system_message,
                max_tokens=2000,
                temperature=0.7,
                provider=LLMProvider.GMS
            )
            
            logger.debug(f"LLM Response: {response[:500]}...")  # ë””ë²„ê¹…ìš© ë¡œê·¸
            
            # ê°•í™”ëœ JSON íŒŒì‹±
            
            try:
                # 1ë‹¨ê³„: ì‘ë‹µ ì •ë¦¬
                cleaned_response = self._clean_json_response(response)
                logger.debug(f"Cleaned response: {cleaned_response[:300]}...")
                
                # 2ë‹¨ê³„: ì§ì ‘ JSON íŒŒì‹± ì‹œë„
                try:
                    questions = json.loads(cleaned_response)
                    if isinstance(questions, list):
                        logger.info("Successfully parsed JSON array directly")
                        return self._process_llm_questions(questions)
                except json.JSONDecodeError as e:
                    logger.debug(f"Direct JSON parsing failed: {e}")
                
                # 3ë‹¨ê³„: ë°°ì—´ ì¶”ì¶œ ì‹œë„
                array_match = re.search(r'\[.*?\]', cleaned_response, re.DOTALL)
                if array_match:
                    try:
                        json_str = array_match.group()
                        questions = json.loads(json_str)
                        if isinstance(questions, list):
                            logger.info("Successfully parsed JSON array from match")
                            return self._process_llm_questions(questions)
                    except json.JSONDecodeError as e:
                        logger.debug(f"Array extraction parsing failed: {e}")
                
                # 4ë‹¨ê³„: ê°œë³„ ê°ì²´ ì¶”ì¶œ ë° ë°°ì—´ êµ¬ì„±
                logger.debug("Attempting individual object parsing")
                questions = []
                # ì¤‘ê´„í˜¸ ê· í˜• ë§ì¶”ê¸° - ë” ì •í™•í•œ ì •ê·œì‹
                objects = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned_response)
                
                for i, obj_str in enumerate(objects):
                    try:
                        # ê°œë³„ ê°ì²´ ì •ë¦¬
                        fixed_obj = self._fix_json_object(obj_str)
                        question = json.loads(fixed_obj)
                        questions.append(question)
                        logger.debug(f"Successfully parsed object {i+1}")
                    except json.JSONDecodeError as e:
                        logger.warning(f"Failed to parse individual JSON object {i+1}: {e}")
                        logger.debug(f"Failed object: {obj_str}")
                        continue
                
                if questions:
                    logger.info(f"Successfully parsed {len(questions)} questions from individual objects")
                    return self._process_llm_questions(questions)
                else:
                    logger.warning("No valid questions found after all parsing attempts")
                    return None
                    
            except Exception as e:
                logger.error(f"JSON parsing completely failed: {e}")
                logger.debug(f"Original response: {response}")
                return None
                
        except Exception as e:
            logger.error(f"LLM quiz generation failed: {e}")
            return None
    
    def _validate_questions(self, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Validate and filter generated questions"""
        validated = []
        
        for i, question in enumerate(questions):
            try:
                # Check required fields
                if not all(key in question for key in ["question", "options", "correct_answer"]):
                    logger.warning(f"Question {i+1} missing required fields")
                    continue
                
                # Validate options
                options = question.get("options", [])
                if not isinstance(options, list) or len(options) < 2:
                    logger.warning(f"Question {i+1} has invalid options")
                    continue
                
                # Validate correct answer
                correct_answer = question.get("correct_answer")
                if not isinstance(correct_answer, int) or correct_answer < 0 or correct_answer >= len(options):
                    logger.warning(f"Question {i+1} has invalid correct answer")
                    continue
                
                # Clean up question text
                clean_question = {
                    "question_text": str(question["question"]).strip(),
                    "options": [str(opt).strip() for opt in options],
                    "correct_answer_index": int(correct_answer),
                    "explanation": str(question.get("explanation", "")).strip(),
                    "category": question.get("category", "general")
                }
                
                validated.append(clean_question)
                
            except Exception as e:
                logger.warning(f"Failed to validate question {i+1}: {e}")
                continue
        
        logger.info(f"Validated {len(validated)} out of {len(questions)} questions")
        return validated
    
    def _generate_quiz_id(self, document_id: str) -> str:
        """Generate unique quiz ID"""
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        return f"quiz_{document_id}_{timestamp}_{unique_id}"
    
    async def get_quiz_by_id(self, quiz_id: str) -> Optional[Dict[str, Any]]:
        """Retrieve quiz by ID"""
        return self.active_quizzes.get(quiz_id)
    
    async def submit_quiz_answers(
        self,
        quiz_id: str,
        user_id: str,
        answers: List[int]
    ) -> Dict[str, Any]:
        """
        Submit and evaluate quiz answers
        
        Args:
            quiz_id: Quiz identifier
            user_id: User identifier
            answers: List of selected answer indices
        
        Returns:
            Dict with evaluation results
        """
        try:
            logger.info(f"Processing quiz submission for user {user_id}")
            
            quiz = self.active_quizzes.get(quiz_id)
            if not quiz:
                return {"success": False, "error": "Quiz not found"}
            
            questions = quiz.get("questions", [])
            if len(answers) != len(questions):
                return {"success": False, "error": "Answer count mismatch"}
            
            # Evaluate answers
            results = []
            correct_count = 0
            
            for i, (question, user_answer) in enumerate(zip(questions, answers)):
                correct_answer = question["correct_answer_index"]
                is_correct = user_answer == correct_answer
                
                if is_correct:
                    correct_count += 1
                
                results.append({
                    "question_index": i,
                    "question": question["question_text"],
                    "user_answer": user_answer,
                    "correct_answer": correct_answer,
                    "is_correct": is_correct,
                    "explanation": question.get("explanation", "")
                })
            
            # Calculate score
            score = (correct_count / len(questions)) * 100
            
            # Store submission record
            submission_record = {
                "quiz_id": quiz_id,
                "user_id": user_id,
                "answers": answers,
                "results": results,
                "score": score,
                "submitted_at": datetime.utcnow().isoformat()
            }
            
            logger.info(f"Quiz submission evaluated: {correct_count}/{len(questions)} correct ({score:.1f}%)")
            
            return {
                "success": True,
                "quiz_id": quiz_id,
                "score": score,
                "correct_count": correct_count,
                "total_questions": len(questions),
                "results": results,
                "message": f"Quiz completed with score {score:.1f}%"
            }
            
        except Exception as e:
            logger.error(f"Quiz submission processing failed: {e}")
            return {"success": False, "error": f"Submission processing failed: {str(e)}"}
    
    async def get_quiz_statistics(self, document_id: str) -> Dict[str, Any]:
        """Get quiz statistics for a document"""
        try:
            # Filter quizzes for the document
            document_quizzes = [
                quiz for quiz in self.active_quizzes.values()
                if quiz.get("document_id") == document_id
            ]
            
            if not document_quizzes:
                return {"success": False, "error": "No quizzes found for document"}
            
            stats = {
                "document_id": document_id,
                "total_quizzes": len(document_quizzes),
                "quiz_types": {
                    "50_percent": len([q for q in document_quizzes if q.get("progress_percentage") == 50]),
                    "100_percent": len([q for q in document_quizzes if q.get("progress_percentage") == 100])
                },
                "difficulty_distribution": {},
                "recent_quizzes": []
            }
            
            # Difficulty distribution
            for quiz in document_quizzes:
                difficulty = quiz.get("difficulty_level", "medium")
                stats["difficulty_distribution"][difficulty] = \
                    stats["difficulty_distribution"].get(difficulty, 0) + 1
            
            # Recent quizzes (last 5)
            sorted_quizzes = sorted(
                document_quizzes,
                key=lambda q: q.get("created_at", ""),
                reverse=True
            )
            
            stats["recent_quizzes"] = [
                {
                    "quiz_id": quiz["quiz_id"],
                    "created_at": quiz.get("created_at"),
                    "question_count": len(quiz.get("questions", [])),
                    "progress_percentage": quiz.get("progress_percentage"),
                    "difficulty_level": quiz.get("difficulty_level")
                }
                for quiz in sorted_quizzes[:5]
            ]
            
            return {"success": True, **stats}
            
        except Exception as e:
            logger.error(f"Failed to get quiz statistics: {e}")
            return {"success": False, "error": str(e)}
    
    async def cleanup_old_quizzes(self, max_age_hours: int = 24):
        """Clean up old quiz records"""
        try:
            current_time = datetime.utcnow()
            cleaned_count = 0
            
            quiz_ids_to_remove = []
            
            for quiz_id, quiz in self.active_quizzes.items():
                created_at_str = quiz.get("created_at")
                if created_at_str:
                    created_at = datetime.fromisoformat(created_at_str)
                    age_hours = (current_time - created_at).total_seconds() / 3600
                    
                    if age_hours > max_age_hours:
                        quiz_ids_to_remove.append(quiz_id)
            
            # Remove old quizzes
            for quiz_id in quiz_ids_to_remove:
                del self.active_quizzes[quiz_id]
                cleaned_count += 1
            
            if cleaned_count > 0:
                logger.info(f"Cleaned up {cleaned_count} old quizzes")
            
        except Exception as e:
            logger.error(f"Quiz cleanup failed: {e}")

    async def cleanup_meeting_quizzes(self, meeting_id: str) -> Dict[str, Any]:
        """
        íŠ¹ì • ë¯¸íŒ…ê³¼ ê´€ë ¨ëœ ëª¨ë“  í€´ì¦ˆ ì‚­ì œ
        
        Args:
            meeting_id: ì‚­ì œí•  ë¯¸íŒ… ID
            
        Returns:
            Dict with cleanup result
        """
        try:
            logger.info(f"Starting quiz cleanup for meeting: {meeting_id}")
            
            quiz_ids_to_remove = []
            
            # í•´ë‹¹ ë¯¸íŒ…ì˜ í€´ì¦ˆë“¤ ì°¾ê¸°
            for quiz_id, quiz in self.active_quizzes.items():
                if quiz.get("meeting_id") == meeting_id:
                    quiz_ids_to_remove.append(quiz_id)
            
            # í€´ì¦ˆ ì‚­ì œ
            cleaned_count = 0
            for quiz_id in quiz_ids_to_remove:
                del self.active_quizzes[quiz_id]
                cleaned_count += 1
                logger.debug(f"Removed quiz: {quiz_id}")
            
            logger.info(f"âœ… Cleaned up {cleaned_count} quizzes for meeting: {meeting_id}")
            
            return {
                "success": True,
                "meeting_id": meeting_id,
                "cleaned_count": cleaned_count,
                "message": f"Successfully cleaned up {cleaned_count} quizzes"
            }
            
        except Exception as e:
            logger.error(f"Quiz cleanup failed for meeting {meeting_id}: {e}")
            return {
                "success": False,
                "meeting_id": meeting_id,
                "error": str(e),
                "message": f"Quiz cleanup failed: {str(e)}"
            }