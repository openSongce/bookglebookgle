"""
Discussion Service for BGBG AI Server
Handles chat moderation and discussion topic generation
"""

from typing import Dict, List, Optional, Any
from loguru import logger

from src.services.mock_discussion_service import MockDiscussionService
from src.services.llm_client import LLMClient
from src.services.vector_db import VectorDBManager
from src.services.bookclub_discussion_manager import BookClubDiscussionManager
from src.config.settings import get_settings
from datetime import datetime


class DiscussionService:
    """Service for discussion AI and chat moderation"""
    
    def __init__(self):
        self.settings = get_settings()
        self.mock_service = MockDiscussionService()
        self.llm_client = LLMClient()
        self.vector_db = None  # Will be initialized later
        self.discussion_manager = None  # Will be initialized later
    
    def initialize_manager(self, vector_db: VectorDBManager):
        """Initialize discussion manager with vector DB"""
        self.vector_db = vector_db
        self.discussion_manager = BookClubDiscussionManager(vector_db)
        logger.info("DiscussionService initialized with BookClubDiscussionManager")
    
    async def start_discussion(
        self, 
        document_id: str,
        meeting_id: str,
        session_id: str, 
        participants: List[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """
        ì±„íŒ…ë°© ê¸°ë°˜ í† ë¡  ì‹œì‘ - LLMì´ ì§„í–‰ìê°€ ë˜ì–´ í† ë¡  ì£¼ì œ ìƒì„±
        
        Args:
            document_id: í† ë¡ í•  ë¬¸ì„œ ID
            meeting_id: ë…ì„œ ëª¨ì„ ID  
            session_id: í† ë¡  ì„¸ì…˜ ID
            participants: ì°¸ê°€ì ëª©ë¡
            
        Returns:
            Dict with success status and discussion topics
        """
        try:
            logger.info(f"ğŸ¯ Starting discussion for document: {document_id}")
            
            # 1. ë²¡í„°DBì—ì„œ ë…ì„œ ëª¨ì„ë³„ ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            document_content = await self.vector_db.get_bookclub_context_for_discussion(
                meeting_id=meeting_id,
                query="í† ë¡  ì£¼ì œ ìƒì„±ì„ ìœ„í•œ ë¬¸ì„œ ë‚´ìš©",
                max_chunks=5
            )
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê²°í•©, ì—†ìœ¼ë©´ ê¸°ë³¸ ê²€ìƒ‰ ì‹œë„
            if document_content:
                document_content = " ".join(document_content)
            else:
                document_content = await self.vector_db.get_document_summary(document_id)
            
            if not document_content:
                return {
                    "success": False,
                    "message": "Document not found in vector database",
                    "discussion_topics": [],
                    "recommended_topic": ""
                }

            # 2. LLMì„ ì‚¬ìš©í•´ í† ë¡  ì£¼ì œ ìƒì„±
            topics_result = await self.generate_discussion_topics(document_content)
            
            if not topics_result["success"]:
                return {
                    "success": False,
                    "message": "Failed to generate discussion topics",
                    "discussion_topics": [],
                    "recommended_topic": ""
                }

            # 3. í† ë¡  ì„¸ì…˜ í™œì„±í™” (ë©”ëª¨ë¦¬ì— ì €ì¥)
            if not hasattr(self, 'active_discussions'):
                self.active_discussions = {}
                
            self.active_discussions[session_id] = {
                "meeting_id": meeting_id,
                "document_id": document_id,
                "started_at": datetime.utcnow(),
                "participants": participants or [],
                "chatbot_active": True
            }
            
            logger.info(f"âœ… Discussion started for session: {session_id}")
            
            return {
                "success": True,
                "message": "Discussion started and topics generated.",
                "discussion_topics": topics_result["topics"],
                "recommended_topic": topics_result["topics"][0] if topics_result["topics"] else ""
            }
            
        except Exception as e:
            logger.error(f"Failed to start discussion: {e}")
            return {
                "success": False,
                "message": f"Discussion start failed: {str(e)}",
                "discussion_topics": [],
                "recommended_topic": ""
            }

    async def end_discussion(
        self, 
        meeting_id: str, 
        session_id: str
    ) -> Dict[str, Any]:
        """
        ì±„íŒ…ë°© í† ë¡  ì¢…ë£Œ - ì±—ë´‡ ë¹„í™œì„±í™”
        
        Args:
            meeting_id: ë…ì„œ ëª¨ì„ ID
            session_id: í† ë¡  ì„¸ì…˜ ID
            
        Returns:
            Dict with success status
        """
        try:
            # í™œì„± í† ë¡ ì—ì„œ ì œê±°
            if hasattr(self, 'active_discussions') and session_id in self.active_discussions:
                del self.active_discussions[session_id]
                logger.info(f"âœ… Discussion ended for session: {session_id}")
                
                return {
                    "success": True,
                    "message": "Discussion ended successfully"
                }
            else:
                return {
                    "success": False,
                    "message": "Discussion session not found"
                }
            
        except Exception as e:
            logger.error(f"Failed to end discussion: {e}")
            return {
                "success": False,
                "message": f"Discussion end failed: {str(e)}"
            }

    async def initialize_discussion(self, init_data: Dict[str, Any]) -> Dict[str, Any]:
        """Initialize discussion session (legacy method)"""
        return await self.mock_service.initialize_discussion(init_data)
    
    async def process_chat_message(
        self, 
        session_id: str,
        message_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ì±„íŒ…ë°©ì—ì„œ í† ë¡  ë©”ì‹œì§€ ì²˜ë¦¬ - LLMì´ í”¼ë“œë°± ë° ì°¸ì—¬ ë…ë ¤
        
        Args:
            session_id: í† ë¡  ì„¸ì…˜ ID
            message_data: ë©”ì‹œì§€ ë°ì´í„° (sender, message, timestamp)
            
        Returns:
            Dict with AI response or None if discussion not active
        """
        try:
            # í† ë¡ ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if not hasattr(self, 'active_discussions') or session_id not in self.active_discussions:
                return {
                    "success": True,
                    "ai_response": None,  # í† ë¡  ë¹„í™œì„±í™” ìƒíƒœì—ì„œëŠ” ì±—ë´‡ ì‘ë‹µ ì—†ìŒ
                    "requires_moderation": False
                }
            
            discussion_info = self.active_discussions[session_id]
            message = message_data.get("message", "")
            sender_nickname = message_data.get("sender", {}).get("nickname", "ì°¸ì—¬ì")
            
            # ë²¡í„°DBì—ì„œ ë…ì„œ ëª¨ì„ë³„ ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì™€ì„œ ë§¥ë½ ì œê³µ
            document_id = discussion_info["document_id"]
            meeting_id = discussion_info["meeting_id"]
            document_content = await self.vector_db.get_bookclub_context_for_discussion(
                meeting_id=meeting_id,
                query=message,
                max_chunks=3
            )
            
            # ê²€ìƒ‰ëœ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ê²°í•©
            if document_content:
                document_content = " ".join(document_content)
            else:
                document_content = await self.vector_db.get_document_summary(document_id)
            
            # LLMìœ¼ë¡œ í† ë¡  ì§„í–‰ì ì‘ë‹µ ìƒì„±
            ai_response = await self.generate_discussion_response(
                message=message,
                sender_nickname=sender_nickname,
                document_content=document_content
            )
            
            return {
                "success": True,
                "ai_response": ai_response,
                "suggested_topics": [],
                "requires_moderation": False
            }
            
        except Exception as e:
            logger.error(f"Chat message processing failed: {e}")
            return {
                "success": False,
                "ai_response": "í† ë¡  ì§„í–‰ì ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "error": str(e)
            }

    async def process_bookclub_chat_message_stream(
        self,
        meeting_id: str,
        session_id: str, 
        message_data: Dict[str, Any]
    ):
        """
        Process book club chat message with streaming response
        ë…ì„œ ëª¨ì„ë³„ í† ë¡  ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
        
        Args:
            meeting_id: ë…ì„œ ëª¨ì„ ID
            session_id: í† ë¡  ì„¸ì…˜ ID
            message_data: ë©”ì‹œì§€ ë°ì´í„°
            
        Yields:
            str: Streaming AI response chunks
        """
        try:
            # Check if discussion is active and chatbot is enabled
            if not self.discussion_manager or not self.discussion_manager.is_chatbot_active(meeting_id, session_id):
                yield "í† ë¡ ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ê±°ë‚˜ AI ì§„í–‰ìê°€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤."
                return
            
            # Update activity
            self.discussion_manager.update_activity(meeting_id, session_id)
            
            # Get book material context
            message = message_data.get("message", "")
            context_chunks = await self.discussion_manager.get_book_material_context(
                meeting_id=meeting_id,
                query=message,
                max_chunks=3
            )
            
            # Generate streaming response with book context
            if self.settings.ai.MOCK_AI_RESPONSES:
                mock_response = await self.mock_service.process_chat_message(message_data)
                yield mock_response.get("ai_response", "Mock í† ë¡  ì§„í–‰ì ì‘ë‹µì…ë‹ˆë‹¤.")
            else:
                async for chunk in self._process_with_bookclub_llm_stream(message_data, context_chunks):
                    yield chunk
                    
        except Exception as e:
            logger.error(f"Book club chat streaming failed: {e}")
            yield "AI í† ë¡  ì§„í–‰ì ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    async def process_chat_message_legacy(self, message_data: Dict[str, Any]) -> Dict[str, Any]:
        """Process chat message and generate AI response (legacy method)"""
        # Mock ëª¨ë“œê°€ ë¹„í™œì„±í™”ë˜ì—ˆì„ ë•Œ ì‹¤ì œ LLM ì‚¬ìš©
        if not self.settings.ai.MOCK_AI_RESPONSES:
            return await self._process_with_llm(message_data)
        else:
            return await self.mock_service.process_chat_message(message_data)

    async def process_chat_message_stream(self, message_data: Dict[str, Any]):
        """
        Process chat message and generate AI response as a stream.
        This is a new async generator method.
        """
        if self.settings.ai.MOCK_AI_RESPONSES:
            # Mock ëª¨ë“œì¼ ê²½ìš°, ê¸°ì¡´ Mock ì„œë¹„ìŠ¤ë¥¼ ìŠ¤íŠ¸ë¦¼ì²˜ëŸ¼ ë°˜í™˜
            mock_response = await self.mock_service.process_chat_message(message_data)
            yield mock_response.get("ai_response", "This is a mock response.")
        else:
            # ì‹¤ì œ LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            try:
                if not hasattr(self.llm_client, 'openrouter_client') or not self.llm_client.openrouter_client:
                    await self.llm_client.initialize()
                    logger.info("âœ… LLM Client initialized in DiscussionService")

                message = message_data.get("message", "")
                sender_nickname = message_data.get("sender_nickname", "ì‚¬ìš©ì")

                system_message = f"""ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI í† ë¡  ì§„í–‰ìì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ë©”ì‹œì§€ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µí•˜ì„¸ìš”:
1. ì‚¬ìš©ìì˜ ì˜ê²¬ì— ê³µê°í•˜ê³  ì¸ì •í•´ì£¼ì„¸ìš”
2. ì¶”ê°€ì ì¸ ê´€ì ì´ë‚˜ ì§ˆë¬¸ì„ ì œì‹œí•˜ì—¬ í† ë¡ ì„ í™œì„±í™”í•˜ì„¸ìš”  
3. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”
4. 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”"""

                prompt = f'{sender_nickname}ë‹˜ì´ ë‹¤ìŒê³¼ ê°™ì´ ë§í–ˆìŠµë‹ˆë‹¤: "{message}"'

                # LLMì˜ ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ
                async for chunk in self.llm_client.generate_completion_stream(
                    prompt=prompt,
                    system_message=system_message,
                    max_tokens=300,
                    temperature=0.8
                ):
                    yield chunk

            except Exception as e:
                logger.error(f"LLM stream processing failed: {e}")
                yield "AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    async def generate_discussion_topics(self, document_content: str) -> Dict[str, Any]:
        """ë²¡í„°DB ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í† ë¡  ì£¼ì œ ìƒì„±"""
        try:
            if not document_content:
                return {"success": False, "error": "Document content is empty"}

            # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” í™•ì¸
            if not hasattr(self.llm_client, 'openrouter_client') or not self.llm_client.openrouter_client:
                await self.llm_client.initialize()
                logger.info("LLM Client initialized for topic generation")

            system_message = """ë‹¹ì‹ ì€ ë…ì„œ ëª¨ì„ì˜ í† ë¡  ì§„í–‰ìì…ë‹ˆë‹¤.
ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì°¸ì—¬ìë“¤ì´ í¥ë¯¸ë¡­ê²Œ í† ë¡ í•  ìˆ˜ ìˆëŠ” ì£¼ì œ 3ê°œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.
ê° ì£¼ì œëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. [ì²« ë²ˆì§¸ í† ë¡  ì£¼ì œ]
2. [ë‘ ë²ˆì§¸ í† ë¡  ì£¼ì œ] 
3. [ì„¸ ë²ˆì§¸ í† ë¡  ì£¼ì œ]"""

            prompt = f"ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í† ë¡  ì£¼ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n{document_content[:1000]}"  # í† í° ì œí•œ

            # LLM í˜¸ì¶œ
            response = await self.llm_client.generate_completion(
                prompt=prompt,
                system_message=system_message,
                max_tokens=300,
                temperature=0.7
            )

            # ì‘ë‹µì—ì„œ ì£¼ì œ ì¶”ì¶œ
            topics = []
            for line in response.strip().split('\n'):
                if line.strip() and (line.strip().startswith(('1.', '2.', '3.')) or len(topics) < 3):
                    topic = line.strip()
                    if '. ' in topic:
                        topic = topic.split('. ', 1)[1]
                    topics.append(topic)
            
            if not topics:
                # ê¸°ë³¸ ì£¼ì œ ì œê³µ
                topics = [
                    "ì´ ë¬¸ì„œì—ì„œ ê°€ì¥ ì¸ìƒ ê¹Šì—ˆë˜ ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "ì‘ê°€ì˜ ì£¼ì¥ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?",
                    "ì´ ë‚´ìš©ì„ ì‹¤ìƒí™œì— ì–´ë–»ê²Œ ì ìš©í•  ìˆ˜ ìˆì„ê¹Œìš”?"
                ]

            return {
                "success": True,
                "topics": topics[:3]  # ìµœëŒ€ 3ê°œ
            }

        except Exception as e:
            logger.error(f"Topic generation failed: {e}")
            return {"success": False, "error": str(e)}

    async def generate_discussion_response(self, message: str, sender_nickname: str, document_content: str) -> str:
        """ì±„íŒ… ë©”ì‹œì§€ì— ëŒ€í•œ í† ë¡  ì§„í–‰ì ì‘ë‹µ ìƒì„±"""
        try:
            # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” í™•ì¸
            if not hasattr(self.llm_client, 'openrouter_client') or not self.llm_client.openrouter_client:
                await self.llm_client.initialize()

            system_message = """ë‹¹ì‹ ì€ ë…ì„œ ëª¨ì„ì˜ ì¹œê·¼í•œ AI í† ë¡  ì§„í–‰ìì…ë‹ˆë‹¤.
ì°¸ì—¬ìì˜ ì˜ê²¬ì— ê³µê°í•˜ê³ , í† ë¡ ì„ í™œì„±í™”í•˜ëŠ” ì‘ë‹µì„ í•´ì£¼ì„¸ìš”.

ì‘ë‹µ ì§€ì¹¨:
1. ì°¸ì—¬ìì˜ ì˜ê²¬ì— ê³µê° í‘œí˜„
2. ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ë‹¤ë¥¸ ê´€ì  ì œì‹œ
3. ë‹¤ë¥¸ ì°¸ì—¬ìë“¤ì˜ ì˜ê²¬ë„ ë“£ê³  ì‹¶ë‹¤ëŠ” í‘œí˜„ í¬í•¨
4. 150ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
5. ì¹œê·¼í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±"""

            prompt = f"""ë¬¸ì„œ ë‚´ìš©: {document_content[:500] if document_content else "ë¬¸ì„œ ë‚´ìš© ì—†ìŒ"}

{sender_nickname}ë‹˜ì´ ë§í–ˆìŠµë‹ˆë‹¤: "{message}"

ìœ„ ë°œì–¸ì— ëŒ€í•´ í† ë¡  ì§„í–‰ìë¡œì„œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

            response = await self.llm_client.generate_completion(
                prompt=prompt,
                system_message=system_message,
                max_tokens=200,
                temperature=0.8
            )

            return response.strip()

        except Exception as e:
            logger.error(f"Discussion response generation failed: {e}")
            return f"{sender_nickname}ë‹˜ì˜ ì˜ê²¬ ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë¶„ë“¤ì€ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?"

    async def generate_location_aware_topics(
        self, 
        document_id: str, 
        target_page: Optional[int] = None,
        target_section: Optional[Dict[str, float]] = None
    ) -> Dict[str, Any]:
        """íŠ¹ì • í˜ì´ì§€ë‚˜ ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í† ë¡  ì£¼ì œ ìƒì„±"""
        try:
            # ìœ„ì¹˜ ì •ë³´ë¥¼ ê³ ë ¤í•œ ë¬¸ì„œ ê²€ìƒ‰
            if target_page:
                document_data = await self.vector_db.get_page_content(document_id, target_page)
            elif target_section:
                # íŠ¹ì • ì¢Œí‘œ ì˜ì—­ì˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰
                document_data = await self.vector_db.get_section_content(
                    document_id, target_section
                )
            else:
                document_data = await self.vector_db.get_document_summary(document_id)
            
            if not document_data:
                return {"success": False, "error": "Document not found"}
            
            # ìœ„ì¹˜ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            location_context = ""
            if target_page:
                location_context = f"í˜ì´ì§€ {target_page}ì˜ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ"
            elif target_section:
                location_context = f"ë¬¸ì„œì˜ íŠ¹ì • ì˜ì—­({target_section})ì„ ì¤‘ì‹¬ìœ¼ë¡œ"
            
            prompt = f"""
            {location_context} ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í† ë¡  ì£¼ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
            
            ë¬¸ì„œ ë‚´ìš©:
            {document_data}
            
            ìš”êµ¬ì‚¬í•­:
            - êµ¬ì²´ì ì´ê³  í† ë¡ í•˜ê¸° ì¢‹ì€ 3-5ê°œì˜ ì£¼ì œ
            - ë…ìë“¤ì´ ê´€ì‹¬ì„ ê°€ì§ˆ ë§Œí•œ ì§ˆë¬¸ í˜•íƒœ
            - ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•œ ì£¼ì œ
            """
            
            topics = await self.llm_client.generate_topics(prompt)
            
            return {
                "success": True,
                "topics": topics,
                "context": location_context,
                "source_location": {
                    "page": target_page,
                    "section": target_section
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to generate location-aware topics: {e}")
            return {"success": False, "error": str(e)}
    
    async def _process_with_bookclub_llm(self, message_data: Dict[str, Any], context_chunks: List[str]) -> Dict[str, Any]:
        """
        Process message with book club context using LLM
        ë…ì„œ ìë£Œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ í† ë¡  ì§„í–‰ì ì‘ë‹µ ìƒì„±
        
        Args:
            message_data: ì‚¬ìš©ì ë©”ì‹œì§€ ë°ì´í„°
            context_chunks: ë…ì„œ ìë£Œ ì»¨í…ìŠ¤íŠ¸ ì²­í¬ë“¤
            
        Returns:
            Dict with AI response based on book content
        """
        try:
            # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            if not hasattr(self.llm_client, 'openrouter_client') or not self.llm_client.openrouter_client:
                await self.llm_client.initialize()
                logger.info("LLM Client initialized for book club discussion")
            
            message = message_data.get("message", "")
            sender_nickname = message_data.get("sender_nickname", "ì°¸ì—¬ì")
            
            # ë…ì„œ ìë£Œ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš° êµ¬ë¶„
            if context_chunks and len(context_chunks) > 0:
                # ë…ì„œ ìë£Œ ê¸°ë°˜ í† ë¡  ì§„í–‰ì ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
                system_message = """ë‹¹ì‹ ì€ ë…ì„œ ëª¨ì„ì˜ ì „ë¬¸ AI í† ë¡  ì§„í–‰ìì…ë‹ˆë‹¤.

ì—­í•  ë° ì§€ì¹¨:
1. ì œê³µëœ ë…ì„œ ìë£Œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í† ë¡ ì„ í™œì„±í™”í•˜ì„¸ìš”
2. ì°¸ì—¬ìì˜ ì˜ê²¬ì— ê³µê°í•˜ê³  ê²©ë ¤í•˜ë©° ì¸ì •í•´ì£¼ì„¸ìš”
3. ë…ì„œ ë‚´ìš©ê³¼ ì—°ê²°ëœ ìƒˆë¡œìš´ ê´€ì ì´ë‚˜ ì§ˆë¬¸ì„ ì œì‹œí•˜ì„¸ìš”
4. ë‹¤ë¥¸ ì°¸ì—¬ìë“¤ì˜ ì°¸ì—¬ë¥¼ ìœ ë„í•˜ëŠ” ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”
5. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ëŒ€í™”í•˜ì„¸ìš”
6. 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
7. ë…ì„œ ìë£Œì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì¸ìš©í•˜ê±°ë‚˜ ì–¸ê¸‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"""
                
                # ë…ì„œ ìë£Œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸
                context_text = "\n\n".join(context_chunks)
                prompt = f"""ë…ì„œ ìë£Œ ë‚´ìš©:
{context_text}

í† ë¡  ìƒí™©:
{sender_nickname}ë‹˜ì´ ë‹¤ìŒê³¼ ê°™ì´ ë§í–ˆìŠµë‹ˆë‹¤: "{message}"

ìœ„ ë…ì„œ ìë£Œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ í† ë¡  ì§„í–‰ìë¡œì„œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
            else:
                # ë…ì„œ ìë£Œê°€ ì—†ëŠ” ê²½ìš° ì¼ë°˜ í† ë¡  ì§„í–‰ì ëª¨ë“œ
                system_message = """ë‹¹ì‹ ì€ ë…ì„œ ëª¨ì„ì˜ ì¹œê·¼í•œ AI í† ë¡  ì§„í–‰ìì…ë‹ˆë‹¤.

ì—­í•  ë° ì§€ì¹¨:
1. ì°¸ì—¬ìì˜ ì˜ê²¬ì— ê³µê°í•˜ê³  ê²©ë ¤í•´ì£¼ì„¸ìš”
2. ë…ì„œì™€ ê´€ë ¨ëœ ì¶”ê°€ì ì¸ ê´€ì ì´ë‚˜ ì§ˆë¬¸ì„ ì œì‹œí•˜ì„¸ìš”
3. ë‹¤ë¥¸ ì°¸ì—¬ìë“¤ì˜ ì°¸ì—¬ë¥¼ ìœ ë„í•˜ì„¸ìš”
4. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”
5. 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”"""
                
                prompt = f"{sender_nickname}ë‹˜ì´ ë‹¤ìŒê³¼ ê°™ì´ ë§í–ˆìŠµë‹ˆë‹¤: \"{message}\""
            
            # LLM í˜¸ì¶œ
            ai_response = await self.llm_client.generate_completion(
                prompt=prompt,
                system_message=system_message,
                max_tokens=300,
                temperature=0.8
            )
            
            return {
                "success": True,
                "message": "Message processed with book club context",
                "ai_response": ai_response,
                "context_used": len(context_chunks) > 0,
                "context_chunks_count": len(context_chunks),
                "suggested_topics": [],
                "requires_moderation": False
            }
            
        except Exception as e:
            logger.error(f"Book club LLM processing failed: {e}")
            # ì‹¤íŒ¨ ì‹œ Mock ì„œë¹„ìŠ¤ë¡œ fallback
            fallback_response = await self.mock_service.process_chat_message(message_data)
            fallback_response["context_used"] = len(context_chunks) > 0
            fallback_response["fallback_used"] = True
            return fallback_response

    async def _process_with_bookclub_llm_stream(self, message_data: Dict[str, Any], context_chunks: List[str]):
        """
        Process message with book club context using streaming LLM
        ë…ì„œ ìë£Œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ ìŠ¤íŠ¸ë¦¬ë° í† ë¡  ì§„í–‰ì ì‘ë‹µ
        
        Args:
            message_data: ì‚¬ìš©ì ë©”ì‹œì§€ ë°ì´í„°
            context_chunks: ë…ì„œ ìë£Œ ì»¨í…ìŠ¤íŠ¸ ì²­í¬ë“¤
            
        Yields:
            str: Streaming AI response chunks
        """
        try:
            # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            if not hasattr(self.llm_client, 'openrouter_client') or not self.llm_client.openrouter_client:
                await self.llm_client.initialize()
                logger.info("LLM Client initialized for book club streaming")
            
            message = message_data.get("message", "")
            sender_nickname = message_data.get("sender_nickname", "ì°¸ì—¬ì")
            
            # ë…ì„œ ìë£Œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            if context_chunks and len(context_chunks) > 0:
                system_message = """ë‹¹ì‹ ì€ ë…ì„œ ëª¨ì„ì˜ ì „ë¬¸ AI í† ë¡  ì§„í–‰ìì…ë‹ˆë‹¤.

ì—­í•  ë° ì§€ì¹¨:
1. ì œê³µëœ ë…ì„œ ìë£Œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í† ë¡ ì„ í™œì„±í™”í•˜ì„¸ìš”
2. ì°¸ì—¬ìì˜ ì˜ê²¬ì— ê³µê°í•˜ê³  ê²©ë ¤í•˜ë©° ì¸ì •í•´ì£¼ì„¸ìš”
3. ë…ì„œ ë‚´ìš©ê³¼ ì—°ê²°ëœ ìƒˆë¡œìš´ ê´€ì ì´ë‚˜ ì§ˆë¬¸ì„ ì œì‹œí•˜ì„¸ìš”
4. ë‹¤ë¥¸ ì°¸ì—¬ìë“¤ì˜ ì°¸ì—¬ë¥¼ ìœ ë„í•˜ëŠ” ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”
5. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ëŒ€í™”í•˜ì„¸ìš”
6. 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
7. ë…ì„œ ìë£Œì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì¸ìš©í•˜ê±°ë‚˜ ì–¸ê¸‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"""
                
                context_text = "\n\n".join(context_chunks)
                prompt = f"""ë…ì„œ ìë£Œ ë‚´ìš©:
{context_text}

í† ë¡  ìƒí™©:
{sender_nickname}ë‹˜ì´ ë‹¤ìŒê³¼ ê°™ì´ ë§í–ˆìŠµë‹ˆë‹¤: "{message}"

ìœ„ ë…ì„œ ìë£Œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ í† ë¡  ì§„í–‰ìë¡œì„œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
            else:
                system_message = """ë‹¹ì‹ ì€ ë…ì„œ ëª¨ì„ì˜ ì¹œê·¼í•œ AI í† ë¡  ì§„í–‰ìì…ë‹ˆë‹¤.

ì—­í•  ë° ì§€ì¹¨:
1. ì°¸ì—¬ìì˜ ì˜ê²¬ì— ê³µê°í•˜ê³  ê²©ë ¤í•´ì£¼ì„¸ìš”
2. ë…ì„œì™€ ê´€ë ¨ëœ ì¶”ê°€ì ì¸ ê´€ì ì´ë‚˜ ì§ˆë¬¸ì„ ì œì‹œí•˜ì„¸ìš”
3. ë‹¤ë¥¸ ì°¸ì—¬ìë“¤ì˜ ì°¸ì—¬ë¥¼ ìœ ë„í•˜ì„¸ìš”
4. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”
5. 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”"""
                
                prompt = f"{sender_nickname}ë‹˜ì´ ë‹¤ìŒê³¼ ê°™ì´ ë§í–ˆìŠµë‹ˆë‹¤: \"{message}\""
            
            # LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            async for chunk in self.llm_client.generate_completion_stream(
                prompt=prompt,
                system_message=system_message,
                max_tokens=300,
                temperature=0.8
            ):
                yield chunk
                
        except Exception as e:
            logger.error(f"Book club LLM streaming failed: {e}")
            # ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ fallback ë©”ì‹œì§€
            if context_chunks and len(context_chunks) > 0:
                yield f"{message_data.get('sender_nickname', '')}ë‹˜ì˜ ì˜ê²¬ ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤. ë…ì„œ ìë£Œì™€ ê´€ë ¨í•´ì„œ ë” ìì„¸íˆ ì´ì•¼ê¸°í•´ë³¼ê¹Œìš”?"
            else:
                yield f"{message_data.get('sender_nickname', '')}ë‹˜ì˜ ìƒê°ì— ê³µê°í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ë¶„ë“¤ì€ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?"

    async def _process_with_llm(self, message_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹¤ì œ LLMì„ ì‚¬ìš©í•œ ì±„íŒ… ì²˜ë¦¬"""
        try:
            # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì²« í˜¸ì¶œ ì‹œ)
            if not hasattr(self.llm_client, 'openrouter_client') or not self.llm_client.openrouter_client:
                await self.llm_client.initialize()
                logger.info("âœ… LLM Client initialized in DiscussionService")
            
            message = message_data.get("message", "")
            sender_nickname = message_data.get("sender_nickname", "ì‚¬ìš©ì")
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
            system_message = f"""ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI í† ë¡  ì§„í–‰ìì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ë©”ì‹œì§€ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µí•˜ì„¸ìš”:
1. ì‚¬ìš©ìì˜ ì˜ê²¬ì— ê³µê°í•˜ê³  ì¸ì •í•´ì£¼ì„¸ìš”
2. ì¶”ê°€ì ì¸ ê´€ì ì´ë‚˜ ì§ˆë¬¸ì„ ì œì‹œí•˜ì—¬ í† ë¡ ì„ í™œì„±í™”í•˜ì„¸ìš”  
3. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”
4. 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”"""
            
            # LLM í˜¸ì¶œ
            ai_response = await self.llm_client.generate_completion(
                prompt=f"{sender_nickname}ë‹˜ì´ ë‹¤ìŒê³¼ ê°™ì´ ë§í–ˆìŠµë‹ˆë‹¤: \"{message}\"",
                system_message=system_message,
                max_tokens=300,
                temperature=0.8
            )
            
            return {
                "success": True,
                "message": "Message processed successfully",
                "ai_response": ai_response,
                "suggested_topics": [],
                "requires_moderation": False
            }
            
        except Exception as e:
            logger.error(f"LLM processing failed: {e}")
            # ì‹¤íŒ¨ ì‹œ Mock ì„œë¹„ìŠ¤ë¡œ fallback
            return await self.mock_service.process_chat_message(message_data)