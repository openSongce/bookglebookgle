"""
Discussion Service for BGBG AI Server
Handles chat moderation and discussion topic generation
"""

from typing import Dict, List, Optional, Any
from loguru import logger

from src.services.mock_discussion_service import MockDiscussionService
from src.services.llm_client import LLMClient
from src.services.vector_db import VectorDBManager
from src.services.bookclub_discussion_manager import BookClubDiscussionManager
from src.services.chat_history_manager import ChatHistoryManager
from src.models.chat_history_models import ChatMessage, MessageType
from src.config.settings import get_settings
from datetime import datetime


class DiscussionService:
    """Service for discussion AI and chat moderation"""
    
    def __init__(self):
        self.settings = get_settings()
        self.mock_service = MockDiscussionService()
        self.llm_client = LLMClient()
        self.vector_db = None  # Will be initialized later
        self.discussion_manager = None
        self.chat_history_manager = ChatHistoryManager()
        self.active_streams = {}  # ì„¸ì…˜ë³„ í™œì„± ìŠ¤íŠ¸ë¦¼ ê´€ë¦¬

    async def initialize_manager(self, vector_db: VectorDBManager):
        self.vector_db = vector_db
        self.discussion_manager = BookClubDiscussionManager(vector_db)
        await self.chat_history_manager.start()
        logger.info("DiscussionService initialized with BookClubDiscussionManager and ChatHistoryManager")

    async def register_stream(self, session_id: str, context: Any):
        if session_id not in self.active_streams:
            self.active_streams[session_id] = []
        self.active_streams[session_id].append(context)
        logger.info(f"Stream registered for session {session_id}. Total streams: {len(self.active_streams[session_id])}")

    async def unregister_stream(self, session_id: str, context: Any):
        if session_id in self.active_streams and context in self.active_streams[session_id]:
            self.active_streams[session_id].remove(context)
            logger.info(f"Stream unregistered for session {session_id}. Remaining streams: {len(self.active_streams[session_id])}")
            if not self.active_streams[session_id]:
                del self.active_streams[session_id]

    async def start_discussion(
        self,
        document_id: str,
        meeting_id: str,
        session_id: str,
        participants: List[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        try:
            logger.info(f"ğŸ¯ Starting discussion for document: {document_id}")
            if self.vector_db is None:
                logger.error("VectorDB not initialized.")
                return {"success": False, "message": "Vector database not initialized."}

            document_content = await self.vector_db.get_bookclub_context_for_discussion(
                meeting_id=meeting_id, query="í† ë¡  ì£¼ì œ ìƒì„±ì„ ìœ„í•œ ë¬¸ì„œ ë‚´ìš©", max_chunks=5
            )
            if not document_content:
                document_content = await self.vector_db.get_document_summary(document_id)
            if not document_content:
                return {"success": False, "message": "Document not found in vector database."}

            topics_result = await self.generate_discussion_topics(" ".join(document_content) if document_content else "")
            if not topics_result["success"]:
                return {"success": False, "message": "Failed to generate discussion topics."}

            if not hasattr(self, 'active_discussions'):
                self.active_discussions = {}
            self.active_discussions[session_id] = {
                "meeting_id": meeting_id,
                "document_id": document_id,
                "started_at": datetime.utcnow(),
                "participants": participants or [],
                "chatbot_active": True
            }
            self.active_streams[session_id] = []  # ìŠ¤íŠ¸ë¦¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
            logger.info(f"âœ… Discussion started for session: {session_id}")
            return {
                "success": True,
                "message": "Discussion started and topics generated.",
                "discussion_topics": topics_result["topics"],
                "recommended_topic": topics_result["topics"][0] if topics_result["topics"] else ""
            }
        except Exception as e:
            logger.error(f"Failed to start discussion: {e}")
            return {"success": False, "message": f"Discussion start failed: {str(e)}"}

    async def end_discussion(
        self,
        meeting_id: str,
        session_id: str
    ) -> Dict[str, Any]:
        try:
            # í™œì„± ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
            if session_id in self.active_streams:
                for context in self.active_streams[session_id]:
                    if not context.done():
                        context.cancel()
                        logger.info(f"Cancelled stream for session {session_id}")
                del self.active_streams[session_id]

            # í™œì„± í† ë¡ ì—ì„œ ì œê±°
            if hasattr(self, 'active_discussions') and session_id in self.active_discussions:
                del self.active_discussions[session_id]
                logger.info(f"âœ… Discussion ended for session: {session_id}")
                return {"success": True, "message": "Discussion ended successfully"}
            else:
                return {"success": False, "message": "Discussion session not found"}
        except Exception as e:
            logger.error(f"Failed to end discussion: {e}")
            return {"success": False, "message": f"Discussion end failed: {str(e)}"}

    
    async def process_chat_message(
        self, 
        session_id: str,
        message_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ì±„íŒ…ë°©ì—ì„œ í† ë¡  ë©”ì‹œì§€ ì²˜ë¦¬ - LLMì´ í”¼ë“œë°± ë° ì°¸ì—¬ ë…ë ¤ (ì±„íŒ… ê¸°ë¡ í†µí•©)
        
        Args:
            session_id: í† ë¡  ì„¸ì…˜ ID
            message_data: ë©”ì‹œì§€ ë°ì´í„° (sender, message, timestamp)
            
        Returns:
            Dict with AI response or None if discussion not active
        """
        try:
            # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ… ê¸°ë¡ì— ì €ì¥
            message = message_data.get("message", "")
            sender_info = message_data.get("sender", {})
            sender_nickname = sender_info.get("nickname", "ì°¸ì—¬ì")
            sender_user_id = sender_info.get("user_id", sender_nickname)
            
            # ChatMessage ê°ì²´ ìƒì„±
            chat_message = ChatMessage(
                message_id="",  # auto-generated
                session_id=session_id,
                user_id=sender_user_id,
                nickname=sender_nickname,
                content=message,
                timestamp=datetime.utcnow(),
                message_type=MessageType.USER
            )
            
            # ì±„íŒ… ê¸°ë¡ì— ì €ì¥
            try:
                await self.chat_history_manager.store_message(session_id, chat_message)
                logger.debug(f"Stored user message in chat history for session {session_id}")
            except Exception as e:
                logger.warning(f"Failed to store message in chat history: {e}")
            
            # í† ë¡ ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if not hasattr(self, 'active_discussions') or session_id not in self.active_discussions:
                return {
                    "success": True,
                    "ai_response": None,  # í† ë¡  ë¹„í™œì„±í™” ìƒíƒœì—ì„œëŠ” ì±—ë´‡ ì‘ë‹µ ì—†ìŒ
                    "requires_moderation": False
                }
            
            discussion_info = self.active_discussions[session_id]
            
            # ì±„íŒ… ê¸°ë¡ì—ì„œ ìµœê·¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ë° AI ì‘ë‹µ í•„ìš”ì„± íŒë‹¨
            try:
                recent_messages = await self.chat_history_manager.get_recent_messages(
                    session_id, limit=10
                )
                
                # ìµœê·¼ ë©”ì‹œì§€ì—ì„œ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ì¹´ìš´íŠ¸ (AI ë©”ì‹œì§€ ì œì™¸)
                user_messages_since_ai = []
                for msg in reversed(recent_messages):
                    if msg.message_type == MessageType.AI:
                        break  # ë§ˆì§€ë§‰ AI ì‘ë‹µ ì´í›„ë¶€í„° ì¹´ìš´íŠ¸
                    if msg.message_type == MessageType.USER:
                        user_messages_since_ai.append(msg)
                
                # ì°¸ì—¬ì ìˆ˜ì— ë”°ë¥¸ AI ì‘ë‹µ ëŒ€ê¸° ì±„íŒ… ìˆ˜ ê³„ì‚°
                participants_count = len(discussion_info.get("participants", []))
                if participants_count <= 1:
                    required_messages = 1  # 1ëª… ì´í•˜: 1ê°œ ë©”ì‹œì§€ í›„ ì‘ë‹µ
                elif participants_count <= 3:
                    required_messages = 2  # 2-3ëª…: 2ê°œ ë©”ì‹œì§€ í›„ ì‘ë‹µ
                else:
                    required_messages = 3  # 4ëª… ì´ìƒ: 3ê°œ ë©”ì‹œì§€ í›„ ì‘ë‹µ (ìµœëŒ€)
                
                # AI ì‘ë‹µì´ í•„ìš”í•œì§€ íŒë‹¨
                should_respond = len(user_messages_since_ai) >= required_messages
                
                logger.debug(f"Participants: {participants_count}, Required messages: {required_messages}, Current user messages: {len(user_messages_since_ai)}")
                
                chat_context = "\n".join([
                    f"{msg.nickname}: {msg.content}" 
                    for msg in recent_messages[-7:]  # ìµœê·¼ 7ê°œ ë©”ì‹œì§€ë§Œ ì‚¬ìš©
                ])
            except Exception as e:
                logger.warning(f"Failed to get chat history context: {e}")
                chat_context = f"{sender_nickname}: {message}"
                # Redis ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ ì‘ë‹µí•˜ì§€ ì•ŠìŒ
                should_respond = False
                user_messages_since_ai = []
                required_messages = 0  # Redis ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
            
            # ë²¡í„°DBì—ì„œ ë…ì„œ ëª¨ì„ë³„ ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì™€ì„œ ë§¥ë½ ì œê³µ
            document_id = discussion_info["document_id"]
            meeting_id = discussion_info["meeting_id"]
            document_content = await self.vector_db.get_bookclub_context_for_discussion(
                meeting_id=meeting_id,
                query=message,
                max_chunks=3
            )
            
            # ê²€ìƒ‰ëœ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ê²°í•©
            if document_content:
                document_content = " ".join(document_content)
            else:
                document_content = await self.vector_db.get_document_summary(document_id)
            
            # AI ì‘ë‹µ ìƒì„± ì—¬ë¶€ ê²°ì •
            ai_response = None
            if should_respond:
                # LLMìœ¼ë¡œ í† ë¡  ì§„í–‰ì ì‘ë‹µ ìƒì„± (ì±„íŒ… ê¸°ë¡ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
                ai_response = await self.generate_discussion_response_with_context(
                    message=message,
                    sender_nickname=sender_nickname,
                    document_content=document_content,
                    chat_context=chat_context
                )
                logger.debug(f"AI response generated after {len(user_messages_since_ai)} user messages (required: {required_messages})")
            else:
                logger.debug(f"AI response skipped - only {len(user_messages_since_ai)} user messages since last AI response (required: {required_messages})")
            
            # AI ì‘ë‹µë„ ì±„íŒ… ê¸°ë¡ì— ì €ì¥
            if ai_response:
                try:
                    ai_message = ChatMessage(
                        message_id="",  # auto-generated
                        session_id=session_id,
                        user_id="ai_moderator",
                        nickname="AI í† ë¡  ì§„í–‰ì",
                        content=ai_response,
                        timestamp=datetime.utcnow(),
                        message_type=MessageType.AI
                    )
                    await self.chat_history_manager.store_message(session_id, ai_message)
                    logger.debug(f"Stored AI response in chat history for session {session_id}")
                except Exception as e:
                    logger.warning(f"Failed to store AI response in chat history: {e}")
            
            return {
                "success": True,
                "ai_response": ai_response,
                "suggested_topics": [],
                "requires_moderation": False,
                "chat_context_used": len(recent_messages) if 'recent_messages' in locals() else 0
            }
            
        except Exception as e:
            logger.error(f"Chat message processing failed: {e}")
            return {
                "success": False,
                "ai_response": "í† ë¡  ì§„í–‰ì ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "error": str(e)
            }

    async def process_bookclub_chat_message_stream(
        self,
        meeting_id: str,
        session_id: str, 
        message_data: Dict[str, Any]
    ):
        """
        Process book club chat message with streaming response (ì±„íŒ… ê¸°ë¡ í†µí•©)
        ë…ì„œ ëª¨ì„ë³„ í† ë¡  ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
        
        Args:
            meeting_id: ë…ì„œ ëª¨ì„ ID
            session_id: í† ë¡  ì„¸ì…˜ ID
            message_data: ë©”ì‹œì§€ ë°ì´í„°
            
        Yields:
            str: Streaming AI response chunks
        """
        try:
            # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ… ê¸°ë¡ì— ì €ì¥
            message = message_data.get("message", "")
            sender_info = message_data.get("sender", {})
            sender_nickname = sender_info.get("nickname", "ì°¸ì—¬ì")
            sender_user_id = sender_info.get("user_id", sender_nickname)
            
            # ChatMessage ê°ì²´ ìƒì„± ë° ì €ì¥
            chat_message = ChatMessage(
                message_id="",  # auto-generated
                session_id=session_id,
                user_id=sender_user_id,
                nickname=sender_nickname,
                content=message,
                timestamp=datetime.utcnow(),
                message_type=MessageType.USER
            )
            
            try:
                await self.chat_history_manager.store_message(session_id, chat_message)
                logger.debug(f"Stored user message in chat history for streaming session {session_id}")
            except Exception as e:
                logger.warning(f"Failed to store message in chat history: {e}")
            
            # Check if discussion is active
            if not hasattr(self, 'active_discussions') or session_id not in self.active_discussions:
                yield "í† ë¡ ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í† ë¡ ì„ ë¨¼ì € ì‹œì‘í•´ì£¼ì„¸ìš”."
                return
            
            # í† ë¡  ì„¸ì…˜ ì •ë³´ í™•ì¸
            discussion_info = self.active_discussions[session_id]
            if not discussion_info.get("chatbot_active", True):
                yield "AI í† ë¡  ì§„í–‰ìê°€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤."
                return
            
            # ì±„íŒ… ê¸°ë¡ì—ì„œ ìµœê·¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ë° AI ì‘ë‹µ í•„ìš”ì„± íŒë‹¨
            try:
                recent_messages = await self.chat_history_manager.get_recent_messages(
                    session_id, limit=10
                )
                
                # ìµœê·¼ ë©”ì‹œì§€ì—ì„œ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ì¹´ìš´íŠ¸ (AI ë©”ì‹œì§€ ì œì™¸)
                user_messages_since_ai = []
                for msg in reversed(recent_messages):
                    if msg.message_type == MessageType.AI:
                        break  # ë§ˆì§€ë§‰ AI ì‘ë‹µ ì´í›„ë¶€í„° ì¹´ìš´íŠ¸
                    if msg.message_type == MessageType.USER:
                        user_messages_since_ai.append(msg)
                
                # ì°¸ì—¬ì ìˆ˜ì— ë”°ë¥¸ AI ì‘ë‹µ ëŒ€ê¸° ì±„íŒ… ìˆ˜ ê³„ì‚°
                participants_count = len(discussion_info.get("participants", []))
                if participants_count <= 1:
                    required_messages = 1  # 1ëª… ì´í•˜: 1ê°œ ë©”ì‹œì§€ í›„ ì‘ë‹µ
                elif participants_count <= 3:
                    required_messages = 2  # 2-3ëª…: 2ê°œ ë©”ì‹œì§€ í›„ ì‘ë‹µ
                else:
                    required_messages = 3  # 4ëª… ì´ìƒ: 3ê°œ ë©”ì‹œì§€ í›„ ì‘ë‹µ (ìµœëŒ€)
                
                # AI ì‘ë‹µì´ í•„ìš”í•œì§€ íŒë‹¨
                should_respond = len(user_messages_since_ai) >= required_messages
                
                logger.debug(f"Streaming - Participants: {participants_count}, Required messages: {required_messages}, Current user messages: {len(user_messages_since_ai)}")
                
                chat_context_chunks = [
                    f"{msg.nickname}: {msg.content}" 
                    for msg in recent_messages[-5:]  # ìµœê·¼ 5ê°œ ë©”ì‹œì§€ë§Œ ì‚¬ìš©
                ]
            except Exception as e:
                logger.warning(f"Failed to get chat history context for streaming: {e}")
                chat_context_chunks = [f"{sender_nickname}: {message}"]
                # Redis ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ ì‘ë‹µí•˜ì§€ ì•ŠìŒ
                should_respond = False
                user_messages_since_ai = []
                required_messages = 0  # Redis ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
            
            # Get book material context from VectorDB
            context_chunks = []
            if self.vector_db:
                try:
                    context_chunks = await self.vector_db.get_bookclub_context_for_discussion(
                        meeting_id=meeting_id,
                        query=message,
                        max_chunks=3
                    )
                except Exception as e:
                    logger.warning(f"Failed to get book context: {e}")
                    context_chunks = []
            
            # AI ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            if not should_respond:
                logger.debug(f"AI streaming response skipped - only {len(user_messages_since_ai)} user messages since last AI response (required: {required_messages})")
                return
            
            logger.debug(f"AI streaming response generated after {len(user_messages_since_ai)} user messages (required: {required_messages})")
            
            # Generate streaming response with book context and chat history
            if self.settings.ai.MOCK_AI_RESPONSES:
                mock_response = await self.mock_service.process_chat_message(message_data)
                yield mock_response.get("ai_response", "Mock í† ë¡  ì§„í–‰ì ì‘ë‹µì…ë‹ˆë‹¤.")
            else:
                # AI ì‘ë‹µì„ ìˆ˜ì§‘í•˜ì—¬ ì±„íŒ… ê¸°ë¡ì— ì €ì¥
                ai_response_chunks = []
                async for chunk in self._process_with_bookclub_llm_stream_with_context(
                    message_data, context_chunks, chat_context_chunks
                ):
                    ai_response_chunks.append(chunk)
                    yield chunk
                
                # ì™„ì„±ëœ AI ì‘ë‹µì„ ì±„íŒ… ê¸°ë¡ì— ì €ì¥
                if ai_response_chunks:
                    try:
                        full_ai_response = "".join(ai_response_chunks)
                        ai_message = ChatMessage(
                            message_id="",  # auto-generated
                            session_id=session_id,
                            user_id="ai_moderator",
                            nickname="AI í† ë¡  ì§„í–‰ì",
                            content=full_ai_response,
                            timestamp=datetime.utcnow(),
                            message_type=MessageType.AI
                        )
                        await self.chat_history_manager.store_message(session_id, ai_message)
                        logger.debug(f"Stored AI streaming response in chat history for session {session_id}")
                    except Exception as e:
                        logger.warning(f"Failed to store AI streaming response in chat history: {e}")
                    
        except Exception as e:
            logger.error(f"Book club chat streaming failed: {e}")
            yield "AI í† ë¡  ì§„í–‰ì ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


    
    async def generate_discussion_topics(self, document_content: str) -> Dict[str, Any]:
        """ë²¡í„°DB ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í† ë¡  ì£¼ì œ ìƒì„±"""
        try:
            if not document_content:
                return {"success": False, "error": "Document content is empty"}

            # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” í™•ì¸ (GMS APIìš©)
            if not hasattr(self.llm_client, 'gms_client') or not self.llm_client.gms_client:
                await self.llm_client.initialize()
                logger.info("LLM Client initialized for topic generation")

            system_message = """ë‹¹ì‹ ì€ ë…ì„œ ëª¨ì„ì˜ í† ë¡  ì§„í–‰ìì…ë‹ˆë‹¤.
ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì°¸ì—¬ìë“¤ì´ í¥ë¯¸ë¡­ê²Œ í† ë¡ í•  ìˆ˜ ìˆëŠ” ì£¼ì œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.
ê° ì£¼ì œëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. [ì²« ë²ˆì§¸ í† ë¡  ì£¼ì œ]
2. [ë‘ ë²ˆì§¸ í† ë¡  ì£¼ì œ] 
3. [ì„¸ ë²ˆì§¸ í† ë¡  ì£¼ì œ]"""

            prompt = f"ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í† ë¡  ì£¼ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n{document_content[:1000]}"  # í† í° ì œí•œ

            # LLM í˜¸ì¶œ (GMS API ì‚¬ìš©)
            from src.services.llm_client import LLMProvider
            response = await self.llm_client.generate_completion(
                prompt=prompt,
                system_message=system_message,
                max_tokens=500,
                temperature=0.7,
                provider=LLMProvider.GMS
            )

            # ì‘ë‹µì—ì„œ ì£¼ì œ ì¶”ì¶œ
            topics = []
            for line in response.strip().split('\n'):
                if line.strip() and (line.strip().startswith(('1.', '2.', '3.')) or len(topics) < 3):
                    topic = line.strip()
                    if '. ' in topic:
                        topic = topic.split('. ', 1)[1]
                    topics.append(topic)
            
            if not topics:
                # ê¸°ë³¸ ì£¼ì œ ì œê³µ
                topics = [
                    "ì´ ë¬¸ì„œì—ì„œ ê°€ì¥ ì¸ìƒ ê¹Šì—ˆë˜ ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "ì‘ê°€ì˜ ì£¼ì¥ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?",
                    "ì´ ë‚´ìš©ì„ ì‹¤ìƒí™œì— ì–´ë–»ê²Œ ì ìš©í•  ìˆ˜ ìˆì„ê¹Œìš”?"
                ]

            return {
                "success": True,
                "topics": topics[:3]  # ìµœëŒ€ 3ê°œ
            }

        except Exception as e:
            logger.error(f"Topic generation failed: {e}")
            return {"success": False, "error": str(e)}

    async def generate_discussion_response(self, message: str, sender_nickname: str, document_content: str) -> str:
        """ì±„íŒ… ë©”ì‹œì§€ì— ëŒ€í•œ í† ë¡  ì§„í–‰ì ì‘ë‹µ ìƒì„±"""
        try:
            # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” í™•ì¸ (GMS APIìš©)
            if not hasattr(self.llm_client, 'gms_client') or not self.llm_client.gms_client:
                await self.llm_client.initialize()

            system_message = """ë‹¹ì‹ ì€ ë…ì„œ ëª¨ì„ì˜ ì¹œê·¼í•œ AI í† ë¡  ì§„í–‰ìì…ë‹ˆë‹¤.
ì°¸ì—¬ìì˜ ì˜ê²¬ì— ê³µê°í•˜ê³ , í† ë¡ ì„ í™œì„±í™”í•˜ëŠ” ì‘ë‹µì„ í•´ì£¼ì„¸ìš”.

ì‘ë‹µ ì§€ì¹¨:
1. ìµœê·¼ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µ
2. ì°¸ì—¬ìì˜ ì˜ê²¬ì— êµ¬ì²´ì ìœ¼ë¡œ ê³µê°í•˜ê³  ì¸ì •
3. ë…ì„œ ë‚´ìš©ê³¼ ì—°ê²°ëœ ìƒˆë¡œìš´ ê´€ì ì´ë‚˜ ì§ˆë¬¸ ì œì‹œ
4. ë‹¤ë¥¸ ì°¸ì—¬ìë“¤ì˜ ì°¸ì—¬ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ë„
5. 150ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ë©´ì„œë„ ì˜ë¯¸ìˆê²Œ ì‘ì„±
6. ì¹œê·¼í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤ ìœ ì§€
7. ëŒ€í™”ê°€ ë°˜ë³µë˜ì§€ ì•Šë„ë¡ ìƒˆë¡œìš´ ê°ë„ì—ì„œ ì ‘ê·¼
8. **ì¤‘ìš”**: ë§¤ ë©”ì‹œì§€ë§ˆë‹¤ ì‘ë‹µí•˜ì§€ ë§ê³ , ì°¸ì—¬ìë“¤ì˜ ë©”ì‹œì§€ê°€ 2-3ê°œ ìŒ“ì¸ í›„ì—ë§Œ ì˜ë¯¸ìˆëŠ” í”¼ë“œë°± ì œê³µ. ë„ˆë¬´ ì ê·¹ì ìœ¼ë¡œ ê°œì…í•˜ì§€ ë§ ê²ƒ
9. ìµœê·¼ ëŒ€í™”ê°€ 2~3ê°œ ì´í•˜ë¡œ ì§§ë‹¤ë©´, ëŒ€í™” ì‹œì‘ì„ ë•ëŠ” ë°°ê²½/ì˜¤í”ˆ ì§ˆë¬¸ì„ í¬í•¨í•˜ê³ , ì•„ì§ ì°¸ì—¬í•˜ì§€ ì•Šì€ ë¶„ë“¤ì„ ë¶€ë“œëŸ½ê²Œ ì´ˆëŒ€
10. ìµœê·¼ 10ê°œ ë‚´ ì°¸ì—¬ ë¹ˆë„ê°€ ë‚®ì€ ì‚¬ëŒ(ë©”ì‹œì§€ 1íšŒ ì´í•˜)ì´ ìˆë‹¤ë©´, ì´ë¦„ì„ ì§ì ‘ ê±°ë¡ í•˜ì§€ ì•Šê³  ëª¨ë‘ì—ê²Œ ì°¸ì—¬ë¥¼ ê¶Œìœ í•˜ëŠ” ì¼ë°˜ ë©”ì‹œì§€ë¥¼ ë§ë¶™ì´ì„¸ìš”
11. ìœ„ì˜ 10ê°œì˜ ê·œì¹™ì„ ì ˆëŒ€ë¡œ ìœ„ë°°í•˜ì§€ ì•Šê¸°"""

            prompt = f"""ë¬¸ì„œ ë‚´ìš©: {document_content[:500] if document_content else "ë¬¸ì„œ ë‚´ìš© ì—†ìŒ"}

{sender_nickname}ë‹˜ì´ ë§í–ˆìŠµë‹ˆë‹¤: "{message}"

ìœ„ ë°œì–¸ì— ëŒ€í•´ í† ë¡  ì§„í–‰ìë¡œì„œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

            # GMS APIë¥¼ ì‚¬ìš©í•œ í† ë¡  ì§„í–‰ì ì‘ë‹µ ìƒì„±
            from src.services.llm_client import LLMProvider
            response = await self.llm_client.generate_completion(
                prompt=prompt,
                system_message=system_message,
                max_tokens=500,
                temperature=0.8,
                provider=LLMProvider.GMS
            )

            return response.strip()

        except Exception as e:
            logger.error(f"Discussion response generation failed: {e}")
            return f"{sender_nickname}ë‹˜ì˜ ì˜ê²¬ ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë¶„ë“¤ì€ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?"
    
    async def generate_discussion_response_with_context(
        self, 
        message: str, 
        sender_nickname: str, 
        document_content: str,
        chat_context: str
    ) -> str:
        """ì±„íŒ… ê¸°ë¡ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ í† ë¡  ì§„í–‰ì ì‘ë‹µ ìƒì„±"""
        try:
            # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” í™•ì¸ (GMS APIìš©)
            if not hasattr(self.llm_client, 'gms_client') or not self.llm_client.gms_client:
                await self.llm_client.initialize()

            system_message = """ë‹¹ì‹ ì€ ë…ì„œ ëª¨ì„ì˜ ì „ë¬¸ AI í† ë¡  ì§„í–‰ìì…ë‹ˆë‹¤.
ìµœê·¼ ëŒ€í™” íë¦„ì„ íŒŒì•…í•˜ê³ , ì°¸ì—¬ìì˜ ì˜ê²¬ì— ë§ì¶¤í˜• ì‘ë‹µì„ í•´ì£¼ì„¸ìš”.

ì‘ë‹µ ì§€ì¹¨:
1. ìµœê·¼ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µ
2. ì°¸ì—¬ìì˜ ì˜ê²¬ì— êµ¬ì²´ì ìœ¼ë¡œ ê³µê°í•˜ê³  ì¸ì •
3. ë…ì„œ ë‚´ìš©ê³¼ ì—°ê²°ëœ ìƒˆë¡œìš´ ê´€ì ì´ë‚˜ ì§ˆë¬¸ ì œì‹œ
4. ë‹¤ë¥¸ ì°¸ì—¬ìë“¤ì˜ ì°¸ì—¬ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ë„
5. 150ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ë©´ì„œë„ ì˜ë¯¸ìˆê²Œ ì‘ì„±
6. ì¹œê·¼í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤ ìœ ì§€
7. ëŒ€í™”ê°€ ë°˜ë³µë˜ì§€ ì•Šë„ë¡ ìƒˆë¡œìš´ ê°ë„ì—ì„œ ì ‘ê·¼
8. **ì¤‘ìš”**: ë§¤ ë©”ì‹œì§€ë§ˆë‹¤ ì‘ë‹µí•˜ì§€ ë§ê³ , ì°¸ì—¬ìë“¤ì˜ ë©”ì‹œì§€ê°€ 2-3ê°œ ìŒ“ì¸ í›„ì—ë§Œ ì˜ë¯¸ìˆëŠ” í”¼ë“œë°± ì œê³µ. ë„ˆë¬´ ì ê·¹ì ìœ¼ë¡œ ê°œì…í•˜ì§€ ë§ ê²ƒ
9. ìµœê·¼ ëŒ€í™”ê°€ 2~3ê°œ ì´í•˜ë¡œ ì§§ë‹¤ë©´, ëŒ€í™” ì‹œì‘ì„ ë•ëŠ” ë°°ê²½/ì˜¤í”ˆ ì§ˆë¬¸ì„ í¬í•¨í•˜ê³ , ì•„ì§ ì°¸ì—¬í•˜ì§€ ì•Šì€ ë¶„ë“¤ì„ ë¶€ë“œëŸ½ê²Œ ì´ˆëŒ€
10. ìµœê·¼ 10ê°œ ë‚´ ì°¸ì—¬ ë¹ˆë„ê°€ ë‚®ì€ ì‚¬ëŒ(ë©”ì‹œì§€ 1íšŒ ì´í•˜)ì´ ìˆë‹¤ë©´, ì´ë¦„ì„ ì§ì ‘ ê±°ë¡ í•˜ì§€ ì•Šê³  ëª¨ë‘ì—ê²Œ ì°¸ì—¬ë¥¼ ê¶Œìœ í•˜ëŠ” ì¼ë°˜ ë©”ì‹œì§€ë¥¼ ë§ë¶™ì´ì„¸ìš”
11. ìœ„ì˜ 10ê°œì˜ ê·œì¹™ì„ ì ˆëŒ€ë¡œ ìœ„ë°°í•˜ì§€ ì•Šê¸°"""

            prompt = f"""ë…ì„œ ìë£Œ ë‚´ìš©:
{document_content[:500] if document_content else "ë…ì„œ ìë£Œ ë‚´ìš© ì—†ìŒ"}

ìµœê·¼ ëŒ€í™” íë¦„:
{chat_context}

í˜„ì¬ ìƒí™©:
{sender_nickname}ë‹˜ì´ ë°©ê¸ˆ ë§í–ˆìŠµë‹ˆë‹¤: "{message}"

ìœ„ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í† ë¡  ì§„í–‰ìë¡œì„œ ìì—°ìŠ¤ëŸ½ê³  ì˜ë¯¸ìˆëŠ” ì‘ë‹µì„ í•´ì£¼ì„¸ìš”."""

            # GMS APIë¥¼ ì‚¬ìš©í•œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í† ë¡  ì§„í–‰ì ì‘ë‹µ ìƒì„±
            from src.services.llm_client import LLMProvider
            response = await self.llm_client.generate_completion(
                prompt=prompt,
                system_message=system_message,
                max_tokens=500,
                temperature=0.8,
                provider=LLMProvider.GMS
            )

            return response.strip()

        except Exception as e:
            logger.error(f"Context-aware discussion response generation failed: {e}")
            # Fallback to basic response
            return await self.generate_discussion_response(message, sender_nickname, document_content)

    

    
    async def _process_with_bookclub_llm_stream_with_context(
        self, 
        message_data: Dict[str, Any], 
        context_chunks: List[str],
        chat_context_chunks: List[str]
    ):
        """
        Process message with book club context and chat history using streaming LLM
        ë…ì„œ ìë£Œ ì»¨í…ìŠ¤íŠ¸ì™€ ì±„íŒ… ê¸°ë¡ì„ í™œìš©í•œ ìŠ¤íŠ¸ë¦¬ë° í† ë¡  ì§„í–‰ì ì‘ë‹µ
        
        Args:
            message_data: ì‚¬ìš©ì ë©”ì‹œì§€ ë°ì´í„°
            context_chunks: ë…ì„œ ìë£Œ ì»¨í…ìŠ¤íŠ¸ ì²­í¬ë“¤
            chat_context_chunks: ì±„íŒ… ê¸°ë¡ ì»¨í…ìŠ¤íŠ¸ ì²­í¬ë“¤
            
        Yields:
            str: Streaming AI response chunks
        """
        try:
            # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            if not hasattr(self.llm_client, 'openrouter_client') or not self.llm_client.openrouter_client:
                await self.llm_client.initialize()
                logger.info("LLM Client initialized for enhanced book club streaming")
            
            message = message_data.get("message", "")
            sender_nickname = message_data.get("sender_nickname", "ì°¸ì—¬ì")
            
            # ì±„íŒ… ê¸°ë¡ê³¼ ë…ì„œ ìë£Œë¥¼ ëª¨ë‘ ê³ ë ¤í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            system_message = """ë‹¹ì‹ ì€ ë…ì„œ ëª¨ì„ì˜ ì „ë¬¸ AI í† ë¡  ì§„í–‰ìì…ë‹ˆë‹¤.
ìµœê·¼ ëŒ€í™” íë¦„ê³¼ ë…ì„œ ìë£Œë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ë§ì¶¤í˜• ì‘ë‹µì„ í•´ì£¼ì„¸ìš”.

ì—­í•  ë° ì§€ì¹¨:
1. ìµœê·¼ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µ
2. ì°¸ì—¬ìì˜ ì˜ê²¬ì— êµ¬ì²´ì ìœ¼ë¡œ ê³µê°í•˜ê³  ì¸ì •
3. ë…ì„œ ë‚´ìš©ê³¼ ì—°ê²°ëœ ìƒˆë¡œìš´ ê´€ì ì´ë‚˜ ì§ˆë¬¸ ì œì‹œ
4. ë‹¤ë¥¸ ì°¸ì—¬ìë“¤ì˜ ì°¸ì—¬ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ë„
5. 150ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ë©´ì„œë„ ì˜ë¯¸ìˆê²Œ ì‘ì„±
6. ì¹œê·¼í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤ ìœ ì§€
7. ëŒ€í™”ê°€ ë°˜ë³µë˜ì§€ ì•Šë„ë¡ ìƒˆë¡œìš´ ê°ë„ì—ì„œ ì ‘ê·¼
8. **ì¤‘ìš”**: ë§¤ ë©”ì‹œì§€ë§ˆë‹¤ ì‘ë‹µí•˜ì§€ ë§ê³ , ì°¸ì—¬ìë“¤ì˜ ë©”ì‹œì§€ê°€ 2-3ê°œ ìŒ“ì¸ í›„ì—ë§Œ ì˜ë¯¸ìˆëŠ” í”¼ë“œë°± ì œê³µ. ë„ˆë¬´ ì ê·¹ì ìœ¼ë¡œ ê°œì…í•˜ì§€ ë§ ê²ƒ
9. ìµœê·¼ ëŒ€í™”ê°€ 2~3ê°œ ì´í•˜ë¡œ ì§§ë‹¤ë©´, ëŒ€í™” ì‹œì‘ì„ ë•ëŠ” ë°°ê²½/ì˜¤í”ˆ ì§ˆë¬¸ì„ í¬í•¨í•˜ê³ , ì•„ì§ ì°¸ì—¬í•˜ì§€ ì•Šì€ ë¶„ë“¤ì„ ë¶€ë“œëŸ½ê²Œ ì´ˆëŒ€
10. ìµœê·¼ 10ê°œ ë‚´ ì°¸ì—¬ ë¹ˆë„ê°€ ë‚®ì€ ì‚¬ëŒ(ë©”ì‹œì§€ 1íšŒ ì´í•˜)ì´ ìˆë‹¤ë©´, ì´ë¦„ì„ ì§ì ‘ ê±°ë¡ í•˜ì§€ ì•Šê³  ëª¨ë‘ì—ê²Œ ì°¸ì—¬ë¥¼ ê¶Œìœ í•˜ëŠ” ì¼ë°˜ ë©”ì‹œì§€ë¥¼ ë§ë¶™ì´ì„¸ìš”
11. ìœ„ì˜ 10ê°œì˜ ê·œì¹™ì„ ì ˆëŒ€ë¡œ ìœ„ë°°í•˜ì§€ ì•Šê¸°"""
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            book_context_text = "\n\n".join(context_chunks) if context_chunks else "ë…ì„œ ìë£Œ ë‚´ìš© ì—†ìŒ"
            chat_context_text = "\n".join(chat_context_chunks) if chat_context_chunks else f"{sender_nickname}: {message}"
            
            prompt = f"""ë…ì„œ ìë£Œ ë‚´ìš©:
{book_context_text}

ìµœê·¼ ëŒ€í™” íë¦„:
{chat_context_text}

í˜„ì¬ ìƒí™©:
{sender_nickname}ë‹˜ì´ ë°©ê¸ˆ ë§í–ˆìŠµë‹ˆë‹¤: "{message}"

ìœ„ ë§¥ë½ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ í† ë¡  ì§„í–‰ìë¡œì„œ ìì—°ìŠ¤ëŸ½ê³  ì˜ë¯¸ìˆëŠ” ì‘ë‹µì„ í•´ì£¼ì„¸ìš”."""
            
            # GMS API ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            from src.services.llm_client import LLMProvider
            async for chunk in self.llm_client.generate_completion_stream(
                prompt=prompt,
                system_message=system_message,
                max_tokens=500,
                temperature=0.8,
                provider=LLMProvider.GMS
            ):
                yield chunk
                
        except Exception as e:
            logger.error(f"Enhanced book club LLM streaming failed: {e}")
            # Fallback to simple message
            sender_nickname = message_data.get('sender_nickname', 'ì°¸ì—¬ì')
            if context_chunks and len(context_chunks) > 0:
                yield f"{sender_nickname}ë‹˜ì˜ ì˜ê²¬ ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤. ë…ì„œ ìë£Œì™€ ê´€ë ¨í•´ì„œ ë” ìì„¸íˆ ì´ì•¼ê¸°í•´ë³¼ê¹Œìš”?"
            else:
                yield f"{sender_nickname}ë‹˜ì˜ ìƒê°ì— ê³µê°í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ë¶„ë“¤ì€ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?"

    
    async def cleanup(self):
        """Clean up resources when shutting down"""
        try:
            if hasattr(self, 'chat_history_manager'):
                await self.chat_history_manager.stop()
                logger.info("ChatHistoryManager stopped")
        except Exception as e:
            logger.error(f"Error during DiscussionService cleanup: {e}")
    
    async def get_chat_history_stats(self, session_id: str) -> Dict[str, Any]:
        """
        Get chat history statistics for a session
        
        Args:
            session_id: Session identifier
            
        Returns:
            Dict[str, Any]: Chat history statistics
        """
        try:
            return await self.chat_history_manager.get_session_stats(session_id)
        except Exception as e:
            logger.error(f"Failed to get chat history stats: {e}")
            return {"session_id": session_id, "error": str(e)}

    async def cleanup_meeting_discussions(self, meeting_id: str) -> Dict[str, Any]:
        """
        íŠ¹ì • ë¯¸íŒ…ê³¼ ê´€ë ¨ëœ ëª¨ë“  í† ë¡  ë°ì´í„° ì‚­ì œ
        
        Args:
            meeting_id: ì‚­ì œí•  ë¯¸íŒ… ID
            
        Returns:
            Dict with cleanup result
        """
        try:
            logger.info(f"Starting discussion cleanup for meeting: {meeting_id}")
            
            cleanup_results = {
                "active_discussions_cleaned": 0,
                "active_streams_cleaned": 0,
                "chat_history_sessions_cleaned": 0
            }
            
            # 1. active_discussionsì—ì„œ í•´ë‹¹ ë¯¸íŒ… ì‚­ì œ
            if hasattr(self, 'active_discussions'):
                sessions_to_remove = []
                for session_id, discussion in self.active_discussions.items():
                    if discussion.get("meeting_id") == meeting_id:
                        sessions_to_remove.append(session_id)
                
                for session_id in sessions_to_remove:
                    del self.active_discussions[session_id]
                    cleanup_results["active_discussions_cleaned"] += 1
                    logger.debug(f"Removed active discussion for session: {session_id}")
            
            # 2. active_streamsì—ì„œ í•´ë‹¹ ë¯¸íŒ…ì˜ ìŠ¤íŠ¸ë¦¼ë“¤ ì •ë¦¬
            sessions_to_remove = []
            for session_id, streams in self.active_streams.items():
                # ì„¸ì…˜ IDë¥¼ í†µí•´ ë¯¸íŒ…ê³¼ ì—°ê´€ëœ ìŠ¤íŠ¸ë¦¼ì¸ì§€ í™•ì¸
                # (ì„¸ì…˜ë³„ë¡œ ë¯¸íŒ… ì •ë³´ë¥¼ ì§ì ‘ í™•ì¸í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, active_discussions ì •ë³´ í™œìš©)
                if hasattr(self, 'active_discussions'):
                    # ì´ë¯¸ ì‚­ì œëœ í† ë¡  ì„¸ì…˜ì˜ ìŠ¤íŠ¸ë¦¼ë“¤ ì •ë¦¬
                    if session_id not in self.active_discussions:
                        sessions_to_remove.append(session_id)
                
                # í™œì„± ìŠ¤íŠ¸ë¦¼ë“¤ ì·¨ì†Œ
                for context in streams:
                    if not context.done():
                        context.cancel()
                        logger.debug(f"Cancelled stream for session {session_id}")
            
            for session_id in sessions_to_remove:
                if session_id in self.active_streams:
                    cleanup_results["active_streams_cleaned"] += len(self.active_streams[session_id])
                    del self.active_streams[session_id]
                    logger.debug(f"Removed active streams for session: {session_id}")
            
            # 3. ì±„íŒ… íˆìŠ¤í† ë¦¬ ì •ë¦¬ (ë¯¸íŒ… IDë¡œ ì—°ê´€ëœ ì„¸ì…˜ë“¤)
            if self.chat_history_manager:
                try:
                    # ëª¨ë“  ì„¸ì…˜ì—ì„œ í•´ë‹¹ ë¯¸íŒ…ê³¼ ê´€ë ¨ëœ ì„¸ì…˜ë“¤ ì°¾ì•„ì„œ ì •ë¦¬
                    # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” chat_history_managerì— ë¯¸íŒ…ë³„ ì •ë¦¬ ë©”ì†Œë“œê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ)
                    
                    # í˜„ì¬ëŠ” active_discussionsì—ì„œ ì°¾ì€ ì„¸ì…˜ë“¤ë§Œ ì •ë¦¬
                    for session_id in sessions_to_remove:
                        try:
                            # ì„¸ì…˜ ë°ì´í„° ì •ë¦¬ - ChatHistoryManagerì— ë©”ì†Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                            if hasattr(self.chat_history_manager, 'clear_session_history'):
                                result = await self.chat_history_manager.clear_session_history(session_id)
                                if result:
                                    cleanup_results["chat_history_sessions_cleaned"] += 1
                                    logger.debug(f"Cleared chat history for session: {session_id}")
                            elif hasattr(self.chat_history_manager, 'delete_session_messages'):
                                # ëŒ€ì²´ ë©”ì†Œë“œ ì‹œë„
                                result = await self.chat_history_manager.delete_session_messages(session_id)
                                if result:
                                    cleanup_results["chat_history_sessions_cleaned"] += 1
                                    logger.debug(f"Deleted session messages for session: {session_id}")
                            else:
                                logger.debug(f"No cleanup method available for chat history in session: {session_id}")
                        except Exception as e:
                            logger.warning(f"Failed to clear chat history for session {session_id}: {e}")
                except Exception as e:
                    logger.warning(f"Chat history cleanup failed: {e}")
            
            total_cleaned = sum(cleanup_results.values())
            logger.info(f"âœ… Discussion cleanup completed for meeting {meeting_id}: "
                       f"{cleanup_results['active_discussions_cleaned']} discussions, "
                       f"{cleanup_results['active_streams_cleaned']} streams, "
                       f"{cleanup_results['chat_history_sessions_cleaned']} chat sessions")
            
            return {
                "success": True,
                "meeting_id": meeting_id,
                "cleanup_results": cleanup_results,
                "total_cleaned": total_cleaned,
                "message": f"Successfully cleaned up {total_cleaned} discussion items"
            }
            
        except Exception as e:
            logger.error(f"Discussion cleanup failed for meeting {meeting_id}: {e}")
            return {
                "success": False,
                "meeting_id": meeting_id,
                "error": str(e),
                "message": f"Discussion cleanup failed: {str(e)}"
            }